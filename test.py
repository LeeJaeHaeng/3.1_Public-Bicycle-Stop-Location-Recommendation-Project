import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
import re
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
import platform
if platform.system() == 'Windows':
    plt.rcParams['font.family'] = 'Malgun Gothic'
else:
    plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

class WorkingSeoulTtareungyiAnalyzer:
    """ì‹¤ì œ ì‘ë™í•˜ëŠ” ì„œìš¸ ë”°ë¦‰ì´ ë°ì´í„° ê¸°ë°˜ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.seoul_usage_raw = None
        self.seoul_station_raw = None
        self.daegu_seogu_raw = None
        self.seoul_district_features = None
        self.models = {}
        self.best_model = None
        
        # íŒŒì¼ ê²½ë¡œ
        self.seoul_usage_path = r"C:\Users\Administrator\Desktop\8_team-Group-Project\tpss_bcycl_od_statnhm_20241102.csv"
        self.seoul_station_path = r"C:\Users\Administrator\Desktop\8_team-Group-Project\ì„œìš¸ì‹œ ë”°ë¦‰ì´ ëŒ€ì—¬ì†Œë³„ ëŒ€ì—¬ë°˜ë‚© ìŠ¹ê°ìˆ˜ ì •ë³´ _20241231.csv"
        self.daegu_seogu_path = r"C:\Users\Administrator\Desktop\8_team-Group-Project\ëŒ€êµ¬ê´‘ì—­ì‹œ  ìˆ˜ì„±êµ¬_ì›”ë³„ì¸êµ¬í˜„í™©.csv"

        
        # ì„œìš¸ ê·¸ë£¹ ë¶„í• 
        self.seoul_groups = {
            'train_A': ['ê°•ë‚¨êµ¬', 'ì„œì´ˆêµ¬', 'ì†¡íŒŒêµ¬', 'ê°•ë™êµ¬', 'ë§ˆí¬êµ¬', 'ìš©ì‚°êµ¬', 'ì„±ë™êµ¬', 'ê´‘ì§„êµ¬'],
            'train_B': ['ê°•ì„œêµ¬', 'ì–‘ì²œêµ¬', 'ì˜ë“±í¬êµ¬', 'êµ¬ë¡œêµ¬', 'ê¸ˆì²œêµ¬', 'ê´€ì•…êµ¬', 'ë™ì‘êµ¬', 'ë…¸ì›êµ¬'],
            'test_C': ['ê°•ë¶êµ¬', 'ë„ë´‰êµ¬', 'ì„±ë¶êµ¬', 'ì¤‘ë‘êµ¬', 'ë™ëŒ€ë¬¸êµ¬', 'ì„œëŒ€ë¬¸êµ¬', 'ì€í‰êµ¬', 'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬']
        }
        
        # ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë™ëª… -> êµ¬ ë§¤í•‘ (ë‹¨ìˆœí™”ëœ ë²„ì „)
        self.dong_to_gu = {
            # ì‹¤ì œ ë”°ë¦‰ì´ ë°ì´í„°ì—ì„œ í™•ì¸ëœ ë™ëª…ë“¤
            'ì ì‹¤': 'ì†¡íŒŒêµ¬', 'ì ì‹¤1ë™': 'ì†¡íŒŒêµ¬', 'ì ì‹¤2ë™': 'ì†¡íŒŒêµ¬', 'ì ì‹¤3ë™': 'ì†¡íŒŒêµ¬', 
            'ì ì‹¤4ë™': 'ì†¡íŒŒêµ¬', 'ì ì‹¤6ë™': 'ì†¡íŒŒêµ¬', 'ì ì‹¤7ë™': 'ì†¡íŒŒêµ¬',
            'ì†¡íŒŒ': 'ì†¡íŒŒêµ¬', 'ì†¡íŒŒ1ë™': 'ì†¡íŒŒêµ¬', 'ì†¡íŒŒ2ë™': 'ì†¡íŒŒêµ¬',
            'í’ë‚©': 'ì†¡íŒŒêµ¬', 'í’ë‚©1ë™': 'ì†¡íŒŒêµ¬', 'í’ë‚©2ë™': 'ì†¡íŒŒêµ¬',
            'ê°€ë½': 'ì†¡íŒŒêµ¬', 'ë¬¸ì •': 'ì†¡íŒŒêµ¬', 'ì¥ì§€': 'ì†¡íŒŒêµ¬',
            
            'ë°©í™”': 'ê°•ì„œêµ¬', 'ë°©í™”1ë™': 'ê°•ì„œêµ¬', 'ë°©í™”2ë™': 'ê°•ì„œêµ¬', 'ë°©í™”3ë™': 'ê°•ì„œêµ¬',
            'ë“±ì´Œ': 'ê°•ì„œêµ¬', 'ë“±ì´Œ1ë™': 'ê°•ì„œêµ¬', 'ë“±ì´Œ2ë™': 'ê°•ì„œêµ¬', 'ë“±ì´Œ3ë™': 'ê°•ì„œêµ¬',
            'í™”ê³¡': 'ê°•ì„œêµ¬', 'í™”ê³¡1ë™': 'ê°•ì„œêµ¬', 'í™”ê³¡2ë™': 'ê°•ì„œêµ¬', 'í™”ê³¡3ë™': 'ê°•ì„œêµ¬',
            'ê°€ì–‘': 'ê°•ì„œêµ¬', 'ê°€ì–‘1ë™': 'ê°•ì„œêµ¬', 'ê°€ì–‘2ë™': 'ê°•ì„œêµ¬', 'ê°€ì–‘3ë™': 'ê°•ì„œêµ¬',
            
            'ì™•ì‹­ë¦¬': 'ì„±ë™êµ¬', 'ì™•ì‹­ë¦¬ë„ì„ ë™': 'ì„±ë™êµ¬', 'ì™•ì‹­ë¦¬2ë™': 'ì„±ë™êµ¬',
            'ì„±ìˆ˜': 'ì„±ë™êµ¬', 'ì„±ìˆ˜1ê°€': 'ì„±ë™êµ¬', 'ì„±ìˆ˜2ê°€': 'ì„±ë™êµ¬',
            'ë§ˆì¥': 'ì„±ë™êµ¬', 'ì‚¬ê·¼': 'ì„±ë™êµ¬', 'í–‰ë‹¹': 'ì„±ë™êµ¬', 'ì‘ë´‰': 'ì„±ë™êµ¬',
            
            'ì¤‘ê³„': 'ë…¸ì›êµ¬', 'ì¤‘ê³„1ë™': 'ë…¸ì›êµ¬', 'ì¤‘ê³„2ë™': 'ë…¸ì›êµ¬', 'ì¤‘ê³„3ë™': 'ë…¸ì›êµ¬', 'ì¤‘ê³„4ë™': 'ë…¸ì›êµ¬',
            'ìƒê³„': 'ë…¸ì›êµ¬', 'ìƒê³„1ë™': 'ë…¸ì›êµ¬', 'ìƒê³„2ë™': 'ë…¸ì›êµ¬', 'ìƒê³„6': 'ë…¸ì›êµ¬', 'ìƒê³„7ë™': 'ë…¸ì›êµ¬',
            'ì›”ê³„': 'ë…¸ì›êµ¬', 'ê³µë¦‰': 'ë…¸ì›êµ¬', 'í•˜ê³„': 'ë…¸ì›êµ¬',
            
            'ëŠ¥ë™': 'ê´‘ì§„êµ¬', 'êµ¬ì˜': 'ê´‘ì§„êµ¬', 'ìì–‘': 'ê´‘ì§„êµ¬', 'í™”ì–‘': 'ê´‘ì§„êµ¬', 'êµ°ì': 'ê´‘ì§„êµ¬',
            
            'ìš©ì‹ ': 'ë™ëŒ€ë¬¸êµ¬', 'ì œê¸°': 'ë™ëŒ€ë¬¸êµ¬', 'ì „ë†': 'ë™ëŒ€ë¬¸êµ¬', 'ë‹µì‹­ë¦¬': 'ë™ëŒ€ë¬¸êµ¬',
            'ì¥ì•ˆ': 'ë™ëŒ€ë¬¸êµ¬', 'ì²­ëŸ‰ë¦¬': 'ë™ëŒ€ë¬¸êµ¬', 'íšŒê¸°': 'ë™ëŒ€ë¬¸êµ¬', 'íœ˜ê²½': 'ë™ëŒ€ë¬¸êµ¬', 'ì´ë¬¸': 'ë™ëŒ€ë¬¸êµ¬',
            
            'ì„œêµ': 'ë§ˆí¬êµ¬', 'í•©ì •': 'ë§ˆí¬êµ¬', 'ë§ì›': 'ë§ˆí¬êµ¬', 'ì—°ë‚¨': 'ë§ˆí¬êµ¬', 'ì„±ì‚°': 'ë§ˆí¬êµ¬',
            'ìƒì•”': 'ë§ˆí¬êµ¬', 'ê³µë•': 'ë§ˆí¬êµ¬', 'ì•„í˜„': 'ë§ˆí¬êµ¬', 'ëŒ€í¥': 'ë§ˆí¬êµ¬',
            
            'ì—¬ì˜': 'ì˜ë“±í¬êµ¬', 'ì—¬ì˜ë™': 'ì˜ë“±í¬êµ¬', 'ë‹¹ì‚°': 'ì˜ë“±í¬êµ¬', 'ë„ë¦¼': 'ì˜ë“±í¬êµ¬',
            'ë¬¸ë˜': 'ì˜ë“±í¬êµ¬', 'ì–‘í‰': 'ì˜ë“±í¬êµ¬', 'ì‹ ê¸¸': 'ì˜ë“±í¬êµ¬', 'ëŒ€ë¦¼': 'ì˜ë“±í¬êµ¬',
            
            'ì‹ ë„ë¦¼': 'êµ¬ë¡œêµ¬', 'êµ¬ë¡œ': 'êµ¬ë¡œêµ¬', 'ê°€ë¦¬ë´‰': 'êµ¬ë¡œêµ¬', 'ê³ ì²™': 'êµ¬ë¡œêµ¬',
            'ê°œë´‰': 'êµ¬ë¡œêµ¬', 'ì˜¤ë¥˜': 'êµ¬ë¡œêµ¬', 'ì²œì™•': 'êµ¬ë¡œêµ¬',
            
            'ê°€ì‚°': 'ê¸ˆì²œêµ¬', 'ë…ì‚°': 'ê¸ˆì²œêµ¬', 'ì‹œí¥': 'ê¸ˆì²œêµ¬',
            
            'ëª©ë™': 'ì–‘ì²œêµ¬', 'ëª©1ë™': 'ì–‘ì²œêµ¬', 'ëª©2ë™': 'ì–‘ì²œêµ¬', 'ëª©3ë™': 'ì–‘ì²œêµ¬',
            'ì‹ ì›”': 'ì–‘ì²œêµ¬', 'ì‹ ì •': 'ì–‘ì²œêµ¬',
            
            'ë³´ë¼ë§¤': 'ê´€ì•…êµ¬', 'ì‹ ë¦¼': 'ê´€ì•…êµ¬', 'ë´‰ì²œ': 'ê´€ì•…êµ¬', 'ë‚™ì„±ëŒ€': 'ê´€ì•…êµ¬',
            
            'ë…¸ëŸ‰ì§„': 'ë™ì‘êµ¬', 'ìƒë„': 'ë™ì‘êµ¬', 'í‘ì„': 'ë™ì‘êµ¬', 'ì‚¬ë‹¹': 'ë™ì‘êµ¬', 'ëŒ€ë°©': 'ë™ì‘êµ¬',
            
            'ë…¼í˜„': 'ê°•ë‚¨êµ¬', 'ì••êµ¬ì •': 'ê°•ë‚¨êµ¬', 'ì²­ë‹´': 'ê°•ë‚¨êµ¬', 'ì‚¼ì„±': 'ê°•ë‚¨êµ¬',
            'ëŒ€ì¹˜': 'ê°•ë‚¨êµ¬', 'ì—­ì‚¼': 'ê°•ë‚¨êµ¬', 'ë„ê³¡': 'ê°•ë‚¨êµ¬', 'ê°œí¬': 'ê°•ë‚¨êµ¬',
            
            'ì„œì´ˆ': 'ì„œì´ˆêµ¬', 'ì„œì´ˆ1ë™': 'ì„œì´ˆêµ¬', 'ì„œì´ˆ2ë™': 'ì„œì´ˆêµ¬', 'ì„œì´ˆ3ë™': 'ì„œì´ˆêµ¬',
            'ì ì›': 'ì„œì´ˆêµ¬', 'ë°˜í¬': 'ì„œì´ˆêµ¬', 'ë°©ë°°': 'ì„œì´ˆêµ¬', 'ì–‘ì¬': 'ì„œì´ˆêµ¬', 'ì–‘ì¬1ë™': 'ì„œì´ˆêµ¬',
            
            'ì„±ë‚´': 'ê°•ë™êµ¬', 'ì²œí˜¸': 'ê°•ë™êµ¬', 'ê°•ì¼': 'ê°•ë™êµ¬', 'ìƒì¼': 'ê°•ë™êµ¬',
            'ëª…ì¼': 'ê°•ë™êµ¬', 'ê³ ë•': 'ê°•ë™êµ¬', 'ì•”ì‚¬': 'ê°•ë™êµ¬', 'ë‘”ì´Œ': 'ê°•ë™êµ¬',
            
            'í›„ì•”': 'ìš©ì‚°êµ¬', 'ìš©ì‚°': 'ìš©ì‚°êµ¬', 'ë‚¨ì˜': 'ìš©ì‚°êµ¬', 'ì²­íŒŒ': 'ìš©ì‚°êµ¬',
            'í•œë‚¨': 'ìš©ì‚°êµ¬', 'ì´íƒœì›': 'ìš©ì‚°êµ¬', 'ì´ì´Œ': 'ìš©ì‚°êµ¬', 'ì„œë¹™ê³ ': 'ìš©ì‚°êµ¬',
            
            'ìˆ˜ìœ ': 'ê°•ë¶êµ¬', 'ë¯¸ì•„': 'ê°•ë¶êµ¬', 'ë²ˆë™': 'ê°•ë¶êµ¬', 'ìš°ì´': 'ê°•ë¶êµ¬',
            
            'ìŒë¬¸': 'ë„ë´‰êµ¬', 'ë°©í•™': 'ë„ë´‰êµ¬', 'ì°½ë™': 'ë„ë´‰êµ¬', 'ë„ë´‰': 'ë„ë´‰êµ¬',
            
            'ì„±ë¶': 'ì„±ë¶êµ¬', 'ì‚¼ì„ ': 'ì„±ë¶êµ¬', 'ëˆì•”': 'ì„±ë¶êµ¬', 'ì•ˆì•”': 'ì„±ë¶êµ¬',
            'ë³´ë¬¸': 'ì„±ë¶êµ¬', 'ì •ë¦‰': 'ì„±ë¶êµ¬', 'ê¸¸ìŒ': 'ì„±ë¶êµ¬', 'ì¢…ì•”': 'ì„±ë¶êµ¬',
            
            'ë©´ëª©': 'ì¤‘ë‘êµ¬', 'ìƒë´‰': 'ì¤‘ë‘êµ¬', 'ì¤‘í™”': 'ì¤‘ë‘êµ¬', 'ë¬µë™': 'ì¤‘ë‘êµ¬', 'ë§ìš°': 'ì¤‘ë‘êµ¬',
            
            'ì¶©í˜„': 'ì„œëŒ€ë¬¸êµ¬', 'ì²œì—°': 'ì„œëŒ€ë¬¸êµ¬', 'ì‹ ì´Œ': 'ì„œëŒ€ë¬¸êµ¬', 'ì—°í¬': 'ì„œëŒ€ë¬¸êµ¬',
            'í™ì œ': 'ì„œëŒ€ë¬¸êµ¬', 'í™ì€': 'ì„œëŒ€ë¬¸êµ¬', 'ë‚¨ê°€ì¢Œ': 'ì„œëŒ€ë¬¸êµ¬', 'ë¶ê°€ì¢Œ': 'ì„œëŒ€ë¬¸êµ¬',
            
            'ì€í‰': 'ì€í‰êµ¬', 'ë…¹ë²ˆ': 'ì€í‰êµ¬', 'ë¶ˆê´‘': 'ì€í‰êµ¬', 'ê°ˆí˜„': 'ì€í‰êµ¬',
            'êµ¬ì‚°': 'ì€í‰êµ¬', 'ëŒ€ì¡°': 'ì€í‰êµ¬', 'ì‘ì•”': 'ì€í‰êµ¬', 'ì—­ì´Œ': 'ì€í‰êµ¬',
            
            'ì²­ìš´': 'ì¢…ë¡œêµ¬', 'ì‚¼ì²­': 'ì¢…ë¡œêµ¬', 'ë¶€ì•”': 'ì¢…ë¡œêµ¬', 'í‰ì°½': 'ì¢…ë¡œêµ¬',
            'ê°€íšŒ': 'ì¢…ë¡œêµ¬', 'ì¢…ë¡œ': 'ì¢…ë¡œêµ¬', 'ì´í™”': 'ì¢…ë¡œêµ¬', 'í˜œí™”': 'ì¢…ë¡œêµ¬',
            'ëª…ë¥œ': 'ì¢…ë¡œêµ¬', 'ì°½ì‹ ': 'ì¢…ë¡œêµ¬', 'ìˆ­ì¸': 'ì¢…ë¡œêµ¬',
            
            'ì†Œê³µ': 'ì¤‘êµ¬', 'íšŒí˜„': 'ì¤‘êµ¬', 'ëª…ë™': 'ì¤‘êµ¬', 'í•„ë™': 'ì¤‘êµ¬', 'ì¥ì¶©': 'ì¤‘êµ¬',
            'ê´‘í¬': 'ì¤‘êµ¬', 'ì„ì§€ë¡œ': 'ì¤‘êµ¬', 'ì‹ ë‹¹': 'ì¤‘êµ¬', 'ë‹¤ì‚°': 'ì¤‘êµ¬', 'ì•½ìˆ˜': 'ì¤‘êµ¬', 'ì²­êµ¬': 'ì¤‘êµ¬'
        }
    
    def load_real_data(self):
        """ì‹¤ì œ ë°ì´í„° ë¡œë”©"""
        print("=== ì‹¤ì œ ë°ì´í„° ë¡œë”© ===")
        
        encodings = ['cp949', 'euc-kr', 'utf-8', 'utf-8-sig']
        
        # 1. ì„œìš¸ ë”°ë¦‰ì´ ì´ìš© ë°ì´í„°
        for encoding in encodings:
            try:
                self.seoul_usage_raw = pd.read_csv(self.seoul_usage_path, encoding=encoding)
                print(f"âœ… ì„œìš¸ ì´ìš© ë°ì´í„° ë¡œë”© ì„±ê³µ ({encoding}): {self.seoul_usage_raw.shape}")
                break
            except:
                continue
        
        # 2. ì„œìš¸ ëŒ€ì—¬ì†Œ ë°ì´í„°  
        for encoding in encodings:
            try:
                self.seoul_station_raw = pd.read_csv(self.seoul_station_path, encoding=encoding)
                print(f"âœ… ì„œìš¸ ëŒ€ì—¬ì†Œ ë°ì´í„° ë¡œë”© ì„±ê³µ ({encoding}): {self.seoul_station_raw.shape}")
                break
            except:
                continue
        
        # 3. ëŒ€êµ¬ ì„œêµ¬ ì¸êµ¬ ë°ì´í„°
        for encoding in encodings:
            try:
                self.daegu_seogu_raw = pd.read_csv(self.daegu_seogu_path, encoding=encoding)
                print(f"âœ… ëŒ€êµ¬ ë°ì´í„° ë¡œë”© ì„±ê³µ ({encoding}): {self.daegu_seogu_raw.shape}")
                break
            except:
                continue
        
        # ì‹¤ì œ ë°ì´í„° ìƒ˜í”Œ í™•ì¸
        if self.seoul_usage_raw is not None:
            print(f"\nğŸ“Š ì„œìš¸ ì´ìš© ë°ì´í„° ìƒ˜í”Œ:")
            sample_stations = self.seoul_usage_raw['ì‹œì‘_ëŒ€ì—¬ì†Œëª…'].dropna().head(10).tolist()
            for station in sample_stations:
                print(f"   {station}")
        
        return True
    
    def extract_gu_info(self, station_name):
        """ëŒ€ì—¬ì†Œëª…ì—ì„œ êµ¬ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë°©ë²•)"""
        if pd.isna(station_name) or station_name == '':
            return None
        
        station_str = str(station_name)
        
        # 1. ì§ì ‘ êµ¬ ì´ë¦„ì´ í¬í•¨ëœ ê²½ìš°
        for gu in ['ê°•ë‚¨êµ¬', 'ê°•ë™êµ¬', 'ê°•ë¶êµ¬', 'ê°•ì„œêµ¬', 'ê´€ì•…êµ¬', 'ê´‘ì§„êµ¬', 'êµ¬ë¡œêµ¬', 'ê¸ˆì²œêµ¬', 
                  'ë…¸ì›êµ¬', 'ë„ë´‰êµ¬', 'ë™ëŒ€ë¬¸êµ¬', 'ë™ì‘êµ¬', 'ë§ˆí¬êµ¬', 'ì„œëŒ€ë¬¸êµ¬', 'ì„œì´ˆêµ¬', 
                  'ì„±ë™êµ¬', 'ì„±ë¶êµ¬', 'ì†¡íŒŒêµ¬', 'ì–‘ì²œêµ¬', 'ì˜ë“±í¬êµ¬', 'ìš©ì‚°êµ¬', 'ì€í‰êµ¬', 
                  'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬', 'ì¤‘ë‘êµ¬']:
            if gu in station_str:
                return gu
        
        # 2. ë™ ì´ë¦„ ë§¤í•‘ (ê°€ì¥ ê¸´ ë§¤ì¹˜ ìš°ì„ )
        matched_dong = None
        max_length = 0
        
        for dong, gu in self.dong_to_gu.items():
            if dong in station_str and len(dong) > max_length:
                matched_dong = dong
                max_length = len(dong)
        
        if matched_dong:
            return self.dong_to_gu[matched_dong]
        
        # 3. ìˆ«ì ì œê±° í›„ ì¬ì‹œë„ (ì˜ˆ: ì ì‹¤6ë™ -> ì ì‹¤ë™)
        clean_name = re.sub(r'\d+', '', station_str)
        for dong, gu in self.dong_to_gu.items():
            if dong in clean_name:
                return gu
        
        return None
    
    def process_seoul_data(self):
        """ì„œìš¸ ë°ì´í„° ì²˜ë¦¬"""
        print("\n=== ì„œìš¸ ë°ì´í„° ì²˜ë¦¬ ===")
        
        # êµ¬ ì •ë³´ ì¶”ì¶œ
        print("ğŸ” êµ¬ ì •ë³´ ì¶”ì¶œ ì¤‘...")
        self.seoul_usage_raw['ì‹œì‘_êµ¬'] = self.seoul_usage_raw['ì‹œì‘_ëŒ€ì—¬ì†Œëª…'].apply(self.extract_gu_info)
        
        # ì¶”ì¶œ ê²°ê³¼ í™•ì¸
        gu_counts = self.seoul_usage_raw['ì‹œì‘_êµ¬'].value_counts()
        print(f"âœ… êµ¬ë³„ ë ˆì½”ë“œ ìˆ˜ (ìƒìœ„ 10ê°œ):")
        print(gu_counts.head(10))
        
        total_extracted = gu_counts.sum()
        total_records = len(self.seoul_usage_raw)
        extraction_rate = total_extracted / total_records * 100
        print(f"ğŸ“Š êµ¬ ì •ë³´ ì¶”ì¶œë¥ : {extraction_rate:.1f}% ({total_extracted:,}/{total_records:,})")
        
        if extraction_rate < 50:
            print("âš ï¸ êµ¬ ì •ë³´ ì¶”ì¶œë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ë” ë§ì€ ë™ ì´ë¦„ì„ ë§¤í•‘ í…Œì´ë¸”ì— ì¶”ê°€í•˜ê² ìŠµë‹ˆë‹¤.")
            self.add_more_dong_mappings()
            # ì¬ì¶”ì¶œ
            self.seoul_usage_raw['ì‹œì‘_êµ¬'] = self.seoul_usage_raw['ì‹œì‘_ëŒ€ì—¬ì†Œëª…'].apply(self.extract_gu_info)
            gu_counts = self.seoul_usage_raw['ì‹œì‘_êµ¬'].value_counts()
            print(f"âœ… ì¬ì¶”ì¶œ í›„ êµ¬ë³„ ë ˆì½”ë“œ ìˆ˜:")
            print(gu_counts.head(10))
        
        # êµ¬ë³„ í†µê³„ ê³„ì‚°
        valid_data = self.seoul_usage_raw[self.seoul_usage_raw['ì‹œì‘_êµ¬'].notna()]
        
        district_stats = valid_data.groupby('ì‹œì‘_êµ¬').agg({
            'ì „ì²´_ê±´ìˆ˜': ['sum', 'mean', 'count', 'std'],
            'ì „ì²´_ì´ìš©_ë¶„': 'mean',
            'ì „ì²´_ì´ìš©_ê±°ë¦¬': 'mean',
            'ê¸°ì¤€_ì‹œê°„ëŒ€': 'mean'
        }).round(2)
        
        district_stats.columns = [
            'ì´_ì´ìš©ê±´ìˆ˜', 'í‰ê· _ì´ìš©ê±´ìˆ˜', 'ì´ìš©_íšŸìˆ˜', 'ì´ìš©ê±´ìˆ˜_í¸ì°¨',
            'í‰ê· _ì´ìš©ì‹œê°„', 'í‰ê· _ì´ìš©ê±°ë¦¬', 'í‰ê· _ì´ìš©ì‹œê°„ëŒ€'
        ]
        
        # ì •ë¥˜ì†Œ ìˆ˜ ê³„ì‚°
        station_counts = valid_data.groupby('ì‹œì‘_êµ¬')['ì‹œì‘_ëŒ€ì—¬ì†Œ_ID'].nunique()
        district_stats['ì •ë¥˜ì†Œìˆ˜'] = station_counts
        
        # ì •ë¥˜ì†Œë‹¹ ì´ìš©ëŸ‰
        district_stats['ì •ë¥˜ì†Œë‹¹_ì´ìš©ëŸ‰'] = district_stats['ì´_ì´ìš©ê±´ìˆ˜'] / district_stats['ì •ë¥˜ì†Œìˆ˜']
        
        district_stats = district_stats.fillna(0)
        
        print(f"\nâœ… êµ¬ë³„ í†µê³„ ì™„ë£Œ:")
        print(district_stats.head())
        
        self.seoul_district_stats = district_stats
        return True
    
    def add_more_dong_mappings(self):
        """ë™ ì´ë¦„ ë§¤í•‘ ì¶”ê°€ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)"""
        # ì‹¤ì œ ë°ì´í„°ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” íŒ¨í„´ë“¤ ì¶”ê°€
        additional_mappings = {
            # íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€
            'ì„ì´Œ': 'ì†¡íŒŒêµ¬', 'ë§ˆì²œ': 'ì†¡íŒŒêµ¬', 'ì˜¤ê¸ˆ': 'ì†¡íŒŒêµ¬', 'ê±°ì—¬': 'ì†¡íŒŒêµ¬',
            'ë°œì‚°': 'ê°•ì„œêµ¬', 'ìš°ì¥ì‚°': 'ê°•ì„œêµ¬', 'í™”ê³¡ë³¸ë™': 'ê°•ì„œêµ¬',
            'ê¸ˆë‚¨': 'ì„±ë™êµ¬', 'ì˜¥ìˆ˜': 'ì„±ë™êµ¬', 'ì†¡ì •': 'ì„±ë™êµ¬', 'ìš©ë‹µ': 'ì„±ë™êµ¬',
            'ì‹ ë‚´': 'ì¤‘ë‘êµ¬', 'ë©´ëª©ë³¸ë™': 'ì¤‘ë‘êµ¬',
            'êµ¬ì˜1ë™': 'ê´‘ì§„êµ¬', 'êµ¬ì˜2ë™': 'ê´‘ì§„êµ¬', 'êµ¬ì˜3ë™': 'ê´‘ì§„êµ¬',
            'ê´‘ì¥': 'ê´‘ì§„êµ¬', 'ì¤‘ê³¡': 'ê´‘ì§„êµ¬',
            # ë” ë§ì€ ë§¤í•‘ ì¶”ê°€...
        }
        
        self.dong_to_gu.update(additional_mappings)
        print(f"ğŸ“ ë™ ì´ë¦„ ë§¤í•‘ ì¶”ê°€: {len(additional_mappings)}ê°œ")
    
    def create_district_features(self):
        """ì„œìš¸ êµ¬ë³„ íŠ¹ì„± ë°ì´í„° ìƒì„±"""
        print("\n=== ì„œìš¸ êµ¬ë³„ íŠ¹ì„± ë°ì´í„° ìƒì„± ===")
        
        all_districts = (self.seoul_groups['train_A'] + 
                        self.seoul_groups['train_B'] + 
                        self.seoul_groups['test_C'])
        
        features_list = []
        
        for district in all_districts:
            # ê·¸ë£¹ ì •ë³´
            if district in self.seoul_groups['train_A']:
                group = 'train_A'
            elif district in self.seoul_groups['train_B']:
                group = 'train_B'
            else:
                group = 'test_C'
            
            # ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            if hasattr(self, 'seoul_district_stats') and district in self.seoul_district_stats.index:
                stats = self.seoul_district_stats.loc[district]
                features = {
                    'êµ¬': district,
                    'ê·¸ë£¹': group,
                    'ì´_ì´ìš©ê±´ìˆ˜': float(stats['ì´_ì´ìš©ê±´ìˆ˜']),
                    'í‰ê· _ì´ìš©ê±´ìˆ˜': float(stats['í‰ê· _ì´ìš©ê±´ìˆ˜']),
                    'ì´ìš©_íšŸìˆ˜': float(stats['ì´ìš©_íšŸìˆ˜']),
                    'ì´ìš©ê±´ìˆ˜_í¸ì°¨': float(stats['ì´ìš©ê±´ìˆ˜_í¸ì°¨']),
                    'í‰ê· _ì´ìš©ì‹œê°„': float(stats['í‰ê· _ì´ìš©ì‹œê°„']),
                    'í‰ê· _ì´ìš©ê±°ë¦¬': float(stats['í‰ê· _ì´ìš©ê±°ë¦¬']),
                    'í‰ê· _ì´ìš©ì‹œê°„ëŒ€': float(stats['í‰ê· _ì´ìš©ì‹œê°„ëŒ€']),
                    'ì •ë¥˜ì†Œìˆ˜': float(stats['ì •ë¥˜ì†Œìˆ˜']),
                    'ì •ë¥˜ì†Œë‹¹_ì´ìš©ëŸ‰': float(stats['ì •ë¥˜ì†Œë‹¹_ì´ìš©ëŸ‰'])
                }
            else:
                # ë°ì´í„°ê°€ ì—†ëŠ” êµ¬ëŠ” í‰ê· ê°’ ì‚¬ìš©
                features = {
                    'êµ¬': district,
                    'ê·¸ë£¹': group,
                    'ì´_ì´ìš©ê±´ìˆ˜': 0.0,
                    'í‰ê· _ì´ìš©ê±´ìˆ˜': 0.0,
                    'ì´ìš©_íšŸìˆ˜': 0.0,
                    'ì´ìš©ê±´ìˆ˜_í¸ì°¨': 0.0,
                    'í‰ê· _ì´ìš©ì‹œê°„': 15.0,
                    'í‰ê· _ì´ìš©ê±°ë¦¬': 2.5,
                    'í‰ê· _ì´ìš©ì‹œê°„ëŒ€': 14.0,
                    'ì •ë¥˜ì†Œìˆ˜': 30.0,
                    'ì •ë¥˜ì†Œë‹¹_ì´ìš©ëŸ‰': 0.0
                }
            
            features_list.append(features)
        
        self.seoul_district_features = pd.DataFrame(features_list)
        
        # ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” êµ¬ë§Œ í‘œì‹œ
        valid_districts = self.seoul_district_features[self.seoul_district_features['ì´_ì´ìš©ê±´ìˆ˜'] > 0]
        print(f"âœ… ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” êµ¬: {len(valid_districts)}ê°œ")
        if len(valid_districts) > 0:
            print(valid_districts[['êµ¬', 'ê·¸ë£¹', 'ì´_ì´ìš©ê±´ìˆ˜', 'ì •ë¥˜ì†Œìˆ˜', 'ì •ë¥˜ì†Œë‹¹_ì´ìš©ëŸ‰']].round(0))
        
        return len(valid_districts) > 0
    
    def train_ml_models(self):
        """ML ëª¨ë¸ í•™ìŠµ (ì‹¤ì œ ë°ì´í„° ë¶€ì¡± ì‹œì—ë„ ê·¸ëŒ€ë¡œ ì§„í–‰)"""
        print("\n=== ML ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ ===")

        # ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” êµ¬ë§Œ ì‚¬ìš©
        valid_data = self.seoul_district_features[self.seoul_district_features['ì´_ì´ìš©ê±´ìˆ˜'] > 0].copy()

        if len(valid_data) < 5:
            print("âš ï¸ ìœ íš¨í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì§€ë§Œ, ì‹¤ì œ ë°ì´í„°ë¡œ ê·¸ëŒ€ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

        # í•™ìŠµìš©ê³¼ ê²€ì¦ìš© ë¶„ë¦¬
        train_data = valid_data[valid_data['ê·¸ë£¹'].isin(['train_A', 'train_B'])].copy()
        test_data = valid_data[valid_data['ê·¸ë£¹'] == 'test_C'].copy()

        print(f"ğŸ“š í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ êµ¬")
        print(f"   í•™ìŠµ êµ¬: {train_data['êµ¬'].tolist()}")
        print(f"ğŸ§ª ê²€ì¦ ë°ì´í„°: {len(test_data)}ê°œ êµ¬")
        print(f"   ê²€ì¦ êµ¬: {test_data['êµ¬'].tolist()}")

        if len(train_data) < 3:
            print("âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ë§¤ìš° ì ì§€ë§Œ, ì‹¤ì œ ë°ì´í„°ë¡œ ê·¸ëŒ€ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

        # í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
        feature_columns = ['ì´_ì´ìš©ê±´ìˆ˜', 'í‰ê· _ì´ìš©ê±´ìˆ˜', 'ì´ìš©_íšŸìˆ˜', 'ì´ìš©ê±´ìˆ˜_í¸ì°¨',
                           'í‰ê· _ì´ìš©ì‹œê°„', 'í‰ê· _ì´ìš©ê±°ë¦¬', 'í‰ê· _ì´ìš©ì‹œê°„ëŒ€', 'ì •ë¥˜ì†Œìˆ˜']

        X_train = train_data[feature_columns].values
        y_train = train_data['ì •ë¥˜ì†Œë‹¹_ì´ìš©ëŸ‰'].values

        print(f"\nğŸ“Š í•™ìŠµ ë°ì´í„° í˜•íƒœ: {X_train.shape}")
        if len(y_train) > 0:
            print(f"íƒ€ê²Ÿ ë²”ìœ„: {y_train.min():.0f} ~ {y_train.max():.0f}")
        else:
            print("âš ï¸ íƒ€ê²Ÿ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        models = {
            'RandomForest': {
                'model': RandomForestRegressor(random_state=42),
                'params': {
                    'n_estimators': [50, 100],
                    'max_depth': [5, 10, None],
                    'min_samples_split': [2, 5],
                    'min_samples_leaf': [1, 2]
                }
            },
            'GradientBoosting': {
                'model': GradientBoostingRegressor(random_state=42),
                'params': {
                    'n_estimators': [50, 100],
                    'learning_rate': [0.1, 0.2],
                    'max_depth': [3, 5],
                    'subsample': [0.8, 1.0]
                }
            },
            'Ridge': {
                'model': Ridge(),
                'params': {
                    'alpha': [0.1, 1.0, 10.0]
                }
            }
        }

        model_results = {}
        best_score = float('-inf')

        for name, model_info in models.items():
            print(f"\nğŸ¤– {name} ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹...")
            try:
                cv = max(2, min(3, len(train_data)))
                grid_search = GridSearchCV(
                    model_info['model'],
                    model_info['params'],
                    cv=cv,
                    scoring='r2',
                    n_jobs=-1
                )

                grid_search.fit(X_train, y_train)

                print(f"âœ… {name} ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
                print(f"âœ… {name} êµì°¨ê²€ì¦ ì ìˆ˜: {grid_search.best_score_:.3f}")

                # ê²€ì¦
                if len(test_data) > 0:
                    X_test = test_data[feature_columns].values
                    y_test = test_data['ì •ë¥˜ì†Œë‹¹_ì´ìš©ëŸ‰'].values
                    y_pred = grid_search.predict(X_test)

                    mse = mean_squared_error(y_test, y_pred)
                    mae = mean_absolute_error(y_test, y_pred)
                    r2 = r2_score(y_test, y_pred)

                    print(f"ğŸ“Š {name} ê²€ì¦ ì„±ëŠ¥:")
                    print(f"   MSE: {mse:.2f}, MAE: {mae:.2f}, RÂ²: {r2:.3f}")

                    comparison_df = pd.DataFrame({
                        'êµ¬': test_data['êµ¬'].values,
                        'ì‹¤ì œê°’': y_test,
                        'ì˜ˆì¸¡ê°’': y_pred,
                        'ì˜¤ì°¨ìœ¨(%)': np.abs(y_test - y_pred) / np.maximum(y_test, 1) * 100
                    })
                    print(f"ğŸ¯ {name} ì˜ˆì¸¡ ê²°ê³¼:")
                    print(comparison_df.round(1))

                    if r2 > best_score:
                        best_score = r2
                        self.best_model = grid_search.best_estimator_

                model_results[name] = {
                    'model': grid_search.best_estimator_,
                    'params': grid_search.best_params_,
                    'score': grid_search.best_score_
                }

            except Exception as e:
                print(f"âŒ {name} ëª¨ë¸ í•™ìŠµ ì˜¤ë¥˜: {e}")

        self.models = model_results
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ RÂ² ì ìˆ˜: {best_score:.3f}")
        return len(model_results) > 0


    
    def create_synthetic_data_for_training(self):
        """ì„ì‹œ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„° ë¶€ì¡±ì‹œ)"""
        print("\nâš ï¸ ì‹¤ì œ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì„ì‹œ ë°ì´í„° ìƒì„±...")
        
        # ì„œìš¸ êµ¬ë³„ ëŒ€ëµì ì¸ íŠ¹ì„± (í˜„ì‹¤ì ì¸ ê°’ë“¤)
        synthetic_data = {
            # Aê·¸ë£¹ (ê³ ì´ìš©ëŸ‰)
            'ê°•ë‚¨êµ¬': {'usage': 45000, 'stations': 120, 'per_station': 375},
            'ì„œì´ˆêµ¬': {'usage': 38000, 'stations': 100, 'per_station': 380},
            'ì†¡íŒŒêµ¬': {'usage': 42000, 'stations': 110, 'per_station': 382},
            'ê°•ë™êµ¬': {'usage': 32000, 'stations': 85, 'per_station': 376},
            'ë§ˆí¬êµ¬': {'usage': 40000, 'stations': 105, 'per_station': 381},
            'ìš©ì‚°êµ¬': {'usage': 35000, 'stations': 90, 'per_station': 389},
            'ì„±ë™êµ¬': {'usage': 36000, 'stations': 95, 'per_station': 379},
            'ê´‘ì§„êµ¬': {'usage': 38000, 'stations': 100, 'per_station': 380},
            
            # Bê·¸ë£¹ (ì¤‘ì´ìš©ëŸ‰)
            'ê°•ì„œêµ¬': {'usage': 28000, 'stations': 80, 'per_station': 350},
            'ì–‘ì²œêµ¬': {'usage': 26000, 'stations': 75, 'per_station': 347},
            'ì˜ë“±í¬êµ¬': {'usage': 34000, 'stations': 90, 'per_station': 378},
            'êµ¬ë¡œêµ¬': {'usage': 25000, 'stations': 70, 'per_station': 357},
            'ê¸ˆì²œêµ¬': {'usage': 22000, 'stations': 65, 'per_station': 338},
            'ê´€ì•…êµ¬': {'usage': 30000, 'stations': 85, 'per_station': 353},
            'ë™ì‘êµ¬': {'usage': 28000, 'stations': 80, 'per_station': 350},
            'ë…¸ì›êµ¬': {'usage': 29000, 'stations': 85, 'per_station': 341},
            
            # Cê·¸ë£¹ (ì €ì´ìš©ëŸ‰)
            'ê°•ë¶êµ¬': {'usage': 18000, 'stations': 60, 'per_station': 300},
            'ë„ë´‰êµ¬': {'usage': 16000, 'stations': 55, 'per_station': 291},
            'ì„±ë¶êµ¬': {'usage': 24000, 'stations': 75, 'per_station': 320},
            'ì¤‘ë‘êµ¬': {'usage': 20000, 'stations': 65, 'per_station': 308},
            'ë™ëŒ€ë¬¸êµ¬': {'usage': 26000, 'stations': 80, 'per_station': 325},
            'ì„œëŒ€ë¬¸êµ¬': {'usage': 25000, 'stations': 75, 'per_station': 333},
            'ì€í‰êµ¬': {'usage': 22000, 'stations': 70, 'per_station': 314},
            'ì¢…ë¡œêµ¬': {'usage': 28000, 'stations': 85, 'per_station': 329},
            'ì¤‘êµ¬': {'usage': 24000, 'stations': 75, 'per_station': 320}
        }
        
        features_list = []
        for district, data in synthetic_data.items():
            # ê·¸ë£¹ ê²°ì •
            if district in self.seoul_groups['train_A']:
                group = 'train_A'
            elif district in self.seoul_groups['train_B']:
                group = 'train_B'
            else:
                group = 'test_C'
            
            # ë…¸ì´ì¦ˆ ì¶”ê°€
            usage_noise = np.random.normal(0, data['usage'] * 0.1)
            station_noise = np.random.randint(-5, 6)
            
            features = {
                'êµ¬': district,
                'ê·¸ë£¹': group,
                'ì´_ì´ìš©ê±´ìˆ˜': max(1000, data['usage'] + usage_noise),
                'í‰ê· _ì´ìš©ê±´ìˆ˜': data['usage'] / data['stations'],
                'ì´ìš©_íšŸìˆ˜': data['usage'] / 30,
                'ì´ìš©ê±´ìˆ˜_í¸ì°¨': data['usage'] * 0.15,
                'í‰ê· _ì´ìš©ì‹œê°„': np.random.normal(15, 2),
                'í‰ê· _ì´ìš©ê±°ë¦¬': np.random.normal(2.5, 0.5),
                'í‰ê· _ì´ìš©ì‹œê°„ëŒ€': np.random.normal(14, 1),
                'ì •ë¥˜ì†Œìˆ˜': max(30, data['stations'] + station_noise),
                'ì •ë¥˜ì†Œë‹¹_ì´ìš©ëŸ‰': data['per_station'] + np.random.normal(0, 20)
            }
            features_list.append(features)
        
        self.seoul_district_features = pd.DataFrame(features_list)
        
        print(f"âœ… ì„ì‹œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(self.seoul_district_features)}ê°œ êµ¬")
        print(self.seoul_district_features.groupby('ê·¸ë£¹')['ì •ë¥˜ì†Œë‹¹_ì´ìš©ëŸ‰'].mean().round(1))
        
        # ì„ì‹œ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ
        return self.train_with_synthetic_data()
    
    def train_with_synthetic_data(self):
        """ì„ì‹œ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ"""
        train_data = self.seoul_district_features[
            self.seoul_district_features['ê·¸ë£¹'].isin(['train_A', 'train_B'])
        ]
        test_data = self.seoul_district_features[
            self.seoul_district_features['ê·¸ë£¹'] == 'test_C'
        ]
        
        feature_columns = ['ì´_ì´ìš©ê±´ìˆ˜', 'í‰ê· _ì´ìš©ê±´ìˆ˜', 'ì´ìš©_íšŸìˆ˜', 'ì´ìš©ê±´ìˆ˜_í¸ì°¨',
                          'í‰ê· _ì´ìš©ì‹œê°„', 'í‰ê· _ì´ìš©ê±°ë¦¬', 'í‰ê· _ì´ìš©ì‹œê°„ëŒ€', 'ì •ë¥˜ì†Œìˆ˜']
        
        X_train = train_data[feature_columns].values
        y_train = train_data['ì •ë¥˜ì†Œë‹¹_ì´ìš©ëŸ‰'].values
        X_test = test_data[feature_columns].values
        y_test = test_data['ì •ë¥˜ì†Œë‹¹_ì´ìš©ëŸ‰'].values
        
        # ê°„ë‹¨í•œ ëª¨ë¸ í•™ìŠµ
        model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
        model.fit(X_train, y_train)
        
        y_pred = model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        
        print(f"ğŸ¤– ì„ì‹œ ë°ì´í„° ëª¨ë¸ ì„±ëŠ¥ RÂ²: {r2:.3f}")
        
        self.best_model = model
        self.models = {'RandomForest': {'model': model, 'score': r2}}
        
        return True
    
    def parse_daegu_population(self):
        """ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì¸êµ¬ ë°ì´í„° íŒŒì‹±"""
        print("\n=== ëŒ€êµ¬ ìˆ˜ì„±ì„±êµ¬ ì¸êµ¬ ë°ì´í„° íŒŒì‹± ===")
        
        try:
            # ë°ì´í„° í™•ì¸ (ëŒ€ì „ ë°ì´í„°ì¸ì§€ ëŒ€êµ¬ ë°ì´í„°ì¸ì§€)
            header_text = str(self.daegu_seogu_raw.iloc[1, 1]) if len(self.daegu_seogu_raw) > 1 else ''
            print(f"ğŸ“Š ë°ì´í„° ì¶œì²˜: {header_text}")
            
            if 'ëŒ€ì „' in header_text:
                print("âš ï¸ ëŒ€ì „ê´‘ì—­ì‹œ ë°ì´í„°ì…ë‹ˆë‹¤. ëŒ€êµ¬ ì„œêµ¬ë¡œ ê°€ì •í•˜ê³  ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # ì´ ì¸êµ¬ìˆ˜ ì¶”ì¶œ (5í–‰ 3ì—´ ê·¼ì²˜)
            total_population = 0
            for i in range(3, 8):  # 5í–‰ ê·¼ì²˜ íƒìƒ‰
                for j in range(2, 6):  # 3ì—´ ê·¼ì²˜ íƒìƒ‰
                    try:
                        val = self.daegu_seogu_raw.iloc[i, j]
                        if pd.notna(val):
                            val_str = str(val).replace(',', '')
                            if val_str.isdigit():
                                num_val = int(val_str)
                                if 100000 < num_val < 1000000:  # í•©ë¦¬ì ì¸ ì¸êµ¬ìˆ˜ ë²”ìœ„
                                    total_population = num_val
                                    break
                    except:
                        continue
                if total_population > 0:
                    break
            
            if total_population == 0:
                total_population = 461087  # í‘œì—ì„œ í™•ì¸ëœ ê°’
            
            print(f"âœ… ëŒ€êµ¬ ìˆ˜ì„±ì„± ì´ ì¸êµ¬ìˆ˜: {total_population:,}ëª…")
            
            self.daegu_features = {
                'ì´ì¸êµ¬ìˆ˜': total_population,
                'ì¸êµ¬ë°€ë„': total_population / 76,  # ìˆ˜ì„±êµ¬ ë©´ì  ì•½ 76kmÂ²
                'ê²½ì œí™œë™ì¸êµ¬ë¹„ìœ¨': 0.65,  # ì¶”ì •
                'ê³ ë ¹í™”ë¹„ìœ¨': 0.15,  # ì¶”ì •
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ ëŒ€êµ¬ ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ê°’ ì‚¬ìš©
            self.daegu_features = {
                'ì´ì¸êµ¬ìˆ˜': 190000,
                'ì¸êµ¬ë°€ë„': 3650,
                'ê²½ì œí™œë™ì¸êµ¬ë¹„ìœ¨': 0.65,
                'ê³ ë ¹í™”ë¹„ìœ¨': 0.15,
            }
            return True
    
    def predict_daegu_stations(self):
        """ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì •ë¥˜ì†Œ ì¶”ì²œ"""
        print("\n=== ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì •ë¥˜ì†Œ ì¶”ì²œ ===")
        
        if self.best_model is None:
            print("âŒ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ì„œìš¸ í‰ê·  ê¸°ë°˜ìœ¼ë¡œ ëŒ€êµ¬ íŠ¹ì„± ì¶”ì •
        # ìˆ«ìí˜• ì—´ë§Œ ì„ íƒí•´ì„œ í‰ê·  ê³„ì‚°
        numeric_cols = ['ì´_ì´ìš©ê±´ìˆ˜', 'í‰ê· _ì´ìš©ê±´ìˆ˜', 'ì´ìš©_íšŸìˆ˜', 'ì´ìš©ê±´ìˆ˜_í¸ì°¨',
                'í‰ê· _ì´ìš©ì‹œê°„', 'í‰ê· _ì´ìš©ê±°ë¦¬', 'í‰ê· _ì´ìš©ì‹œê°„ëŒ€', 'ì •ë¥˜ì†Œìˆ˜']

        seoul_avg = self.seoul_district_features[
            self.seoul_district_features['ê·¸ë£¹'].isin(['train_A', 'train_B'])
            ][numeric_cols].mean()

        
        daegu_population = self.daegu_features['ì´ì¸êµ¬ìˆ˜']
        seoul_avg_population = 450000  # ì„œìš¸ êµ¬ í‰ê·  ì¸êµ¬
        
        # ì¸êµ¬ ë¹„ë¡€ + ì§€ì—­ íŠ¹ì„± ë³´ì •
        population_ratio = (daegu_population / seoul_avg_population) * 0.4  # ëŒ€êµ¬ëŠ” ì´ìš©ë¥  ë‚®ìŒ
        
        # ëŒ€êµ¬ íŠ¹ì„± ë²¡í„° ìƒì„±
        daegu_features_array = np.array([[
            seoul_avg['ì´_ì´ìš©ê±´ìˆ˜'] * population_ratio,
            seoul_avg['í‰ê· _ì´ìš©ê±´ìˆ˜'] * population_ratio,
            seoul_avg['ì´ìš©_íšŸìˆ˜'] * population_ratio,
            seoul_avg['ì´ìš©ê±´ìˆ˜_í¸ì°¨'] * population_ratio,
            seoul_avg['í‰ê· _ì´ìš©ì‹œê°„'],
            seoul_avg['í‰ê· _ì´ìš©ê±°ë¦¬'],
            seoul_avg['í‰ê· _ì´ìš©ì‹œê°„ëŒ€'],
            seoul_avg['ì •ë¥˜ì†Œìˆ˜'] * 0.5  # ëŒ€êµ¬ëŠ” ì •ë¥˜ì†Œ ì ê²Œ ì‹œì‘
        ]])
        
        # ì˜ˆì¸¡
        predicted_usage_per_station = self.best_model.predict(daegu_features_array)[0]
        predicted_usage_per_station = max(150, predicted_usage_per_station)  # ìµœì†Œê°’ ë³´ì¥
        
        print(f"ğŸ¯ ì˜ˆì¸¡ëœ ì •ë¥˜ì†Œë‹¹ ì›” ì´ìš©ëŸ‰: {predicted_usage_per_station:.0f}ê±´")
        
        # ì ì • ì •ë¥˜ì†Œ ìˆ˜ ê³„ì‚°
        target_total_usage = 12000  # ëª©í‘œ ì›”ê°„ ì´ ì´ìš©ëŸ‰
        optimal_stations = max(15, min(25, int(target_total_usage / predicted_usage_per_station)))
        
        print(f"ğŸ“Š ì¶”ì²œ ì •ë¥˜ì†Œ ìˆ˜: {optimal_stations}ê°œ")
        print(f"ğŸ“ˆ ì˜ˆìƒ ì›”ê°„ ì´ ì´ìš©ëŸ‰: {optimal_stations * predicted_usage_per_station:,.0f}ê±´")
        
        # ìƒì„¸ ìœ„ì¹˜ ì¶”ì²œ
        detailed_locations = [
    {'ìˆœìœ„': 1, 'ìœ„ì¹˜': 'ìˆ˜ì„±ëª»ì—­ 1ë²ˆ ì¶œêµ¬', 'ì¹´í…Œê³ ë¦¬': 'êµí†µì¤‘ì‹¬ì§€', 'ì˜ˆìƒì´ìš©': 'ë§¤ìš°ë†’ìŒ', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë‘ì‚°ë™'},
    {'ìˆœìœ„': 2, 'ìœ„ì¹˜': 'ìˆ˜ì„±êµ¬ì²­ì—­ 2ë²ˆ ì¶œêµ¬', 'ì¹´í…Œê³ ë¦¬': 'êµí†µì¤‘ì‹¬ì§€', 'ì˜ˆìƒì´ìš©': 'ë§¤ìš°ë†’ìŒ', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì¤‘ë™'},
    {'ìˆœìœ„': 3, 'ìœ„ì¹˜': 'ë²”ì–´ì—­ 4ë²ˆ ì¶œêµ¬', 'ì¹´í…Œê³ ë¦¬': 'êµí†µì¤‘ì‹¬ì§€', 'ì˜ˆìƒì´ìš©': 'ë†’ìŒ', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë²”ì–´ë™'},
    {'ìˆœìœ„': 4, 'ìœ„ì¹˜': 'ëŒ€êµ¬ìˆ˜ì„±êµ¬ì²­', 'ì¹´í…Œê³ ë¦¬': 'í–‰ì •ê¸°ê´€', 'ì˜ˆìƒì´ìš©': 'ë†’ìŒ', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ìˆ˜ì„±ëª»ê¸¸'},
    {'ìˆœìœ„': 5, 'ìœ„ì¹˜': 'ê³„ëª…ëŒ€ ëŒ€ëª…ìº í¼ìŠ¤ ìˆ˜ì„±ê´€', 'ì¹´í…Œê³ ë¦¬': 'êµìœ¡ê¸°ê´€', 'ì˜ˆìƒì´ìš©': 'ë†’ìŒ', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë‹¬êµ¬ë²ŒëŒ€ë¡œ'},
    {'ìˆœìœ„': 6, 'ìœ„ì¹˜': 'ì‹ ì„¸ê³„ë°±í™”ì  ë™ëŒ€êµ¬ì ', 'ì¹´í…Œê³ ë¦¬': 'ìƒì—…ì‹œì„¤', 'ì˜ˆìƒì´ìš©': 'ë†’ìŒ', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë™ëŒ€êµ¬ë¡œ'},
    {'ìˆœìœ„': 7, 'ìœ„ì¹˜': 'ìˆ˜ì„±ì•„íŠ¸í”¼ì•„', 'ì¹´í…Œê³ ë¦¬': 'ë¬¸í™”ì‹œì„¤', 'ì˜ˆìƒì´ìš©': 'ì¤‘ê°„', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë¬´í•™ë¡œ'},
    {'ìˆœìœ„': 8, 'ìœ„ì¹˜': 'ê²½ë¶ê³ ë“±í•™êµ ì •ë¬¸', 'ì¹´í…Œê³ ë¦¬': 'êµìœ¡ê¸°ê´€', 'ì˜ˆìƒì´ìš©': 'ì¤‘ê°„', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ìˆ˜ì„±ë™'},
    {'ìˆœìœ„': 9, 'ìœ„ì¹˜': 'ìˆ˜ì„±ëª» ì‚°ì±…ë¡œ ì…êµ¬', 'ì¹´í…Œê³ ë¦¬': 'ê³µì›', 'ì˜ˆìƒì´ìš©': 'ì¤‘ê°„', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë‘ì‚°ë™'},
    {'ìˆœìœ„': 10, 'ìœ„ì¹˜': 'ìˆ˜ì„±êµ¬ë³´ê±´ì†Œ', 'ì¹´í…Œê³ ë¦¬': 'í–‰ì •ê¸°ê´€', 'ì˜ˆìƒì´ìš©': 'ì¤‘ê°„', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë™ëŒ€êµ¬ë¡œ'},
    {'ìˆœìœ„': 11, 'ìœ„ì¹˜': 'ë¡¯ë°ë°±í™”ì  ëŒ€êµ¬ì ', 'ì¹´í…Œê³ ë¦¬': 'ìƒì—…ì‹œì„¤', 'ì˜ˆìƒì´ìš©': 'ì¤‘ê°„', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë‹¬êµ¬ë²ŒëŒ€ë¡œ'},
    {'ìˆœìœ„': 12, 'ìœ„ì¹˜': 'ìˆ˜ì„±ìœ ì›ì§€ ì…êµ¬', 'ì¹´í…Œê³ ë¦¬': 'ë¬¸í™”ì‹œì„¤', 'ì˜ˆìƒì´ìš©': 'ì¤‘ê°„', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë‘ì‚°ë™'},
    {'ìˆœìœ„': 13, 'ìœ„ì¹˜': 'ë²”ì–´ë„ì„œê´€', 'ì¹´í…Œê³ ë¦¬': 'ë¬¸í™”ì‹œì„¤', 'ì˜ˆìƒì´ìš©': 'ë‚®ìŒ', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë²”ì–´ë™'},
    {'ìˆœìœ„': 14, 'ìœ„ì¹˜': 'í™©ê¸ˆë™ ì£¼ë¯¼ì„¼í„°', 'ì¹´í…Œê³ ë¦¬': 'í–‰ì •ê¸°ê´€', 'ì˜ˆìƒì´ìš©': 'ë‚®ìŒ', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ í™©ê¸ˆë™'},
    {'ìˆœìœ„': 15, 'ìœ„ì¹˜': 'ìˆ˜ì„±ì‹œì¥ ì…êµ¬', 'ì¹´í…Œê³ ë¦¬': 'ìƒì—…ì‹œì„¤', 'ì˜ˆìƒì´ìš©': 'ë‚®ìŒ', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ìˆ˜ì„±ë™'},
    {'ìˆœìœ„': 16, 'ìœ„ì¹˜': 'ë§Œì´Œ3ë™ ì•„íŒŒíŠ¸ë‹¨ì§€ ì•', 'ì¹´í…Œê³ ë¦¬': 'ì£¼ê±°ì§€ì—­', 'ì˜ˆìƒì´ìš©': 'ì¤‘ê°„', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë§Œì´Œë™'},
    {'ìˆœìœ„': 17, 'ìœ„ì¹˜': 'ê³ ì‚°2ë™ ì•„íŒŒíŠ¸ë‹¨ì§€ ì•', 'ì¹´í…Œê³ ë¦¬': 'ì£¼ê±°ì§€ì—­', 'ì˜ˆìƒì´ìš©': 'ì¤‘ê°„', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ê³ ì‚°ë™'},
    {'ìˆœìœ„': 18, 'ìœ„ì¹˜': 'ë“¤ì•ˆê¸¸ ë¨¹ê±°ë¦¬íƒ€ìš´ ì…êµ¬', 'ì¹´í…Œê³ ë¦¬': 'ìƒì—…ì‹œì„¤', 'ì˜ˆìƒì´ìš©': 'ë‚®ìŒ', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ìƒë™'},
    {'ìˆœìœ„': 19, 'ìœ„ì¹˜': 'ì§€ì‚°ë™ ì²´ìœ¡ê³µì› ì…êµ¬', 'ì¹´í…Œê³ ë¦¬': 'ê³µì›', 'ì˜ˆìƒì´ìš©': 'ë‚®ìŒ', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì§€ì‚°ë™'},
    {'ìˆœìœ„': 20, 'ìœ„ì¹˜': 'ì‚¼ë•ì´ˆë“±í•™êµ í›„ë¬¸ ì•', 'ì¹´í…Œê³ ë¦¬': 'êµìœ¡ê¸°ê´€', 'ì˜ˆìƒì´ìš©': 'ë‚®ìŒ', 'ì£¼ì†Œ': 'ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë²”ì–´ë™'},
]

        
        recommended_locations = detailed_locations[:optimal_stations]
        
        # ì›”ë³„ ì˜ˆì¸¡ (ê³„ì ˆì„± ê³ ë ¤)
        seasonal_factors = [0.7, 0.7, 1.1, 1.2, 1.3, 0.9, 0.8, 0.8, 1.1, 1.2, 1.0, 0.8]
        monthly_predictions = []
        
        for month in range(12):
            monthly_usage = predicted_usage_per_station * seasonal_factors[month]
            monthly_predictions.append({
                'ì›”': month + 1,
                'ì •ë¥˜ì†Œë‹¹_ì˜ˆìƒì´ìš©': int(monthly_usage),
                'ì´_ì˜ˆìƒì´ìš©': int(monthly_usage * optimal_stations)
            })
        
        results = {
            'optimal_stations': optimal_stations,
            'predicted_usage_per_station': int(predicted_usage_per_station),
            'total_monthly_usage': int(optimal_stations * predicted_usage_per_station),
            'recommended_locations': recommended_locations,
            'monthly_predictions': monthly_predictions
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ¯ === ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì •ë¥˜ì†Œ ì¶”ì²œ ìµœì¢… ê²°ê³¼ ===")
        print(f"ğŸ“Š ì¶”ì²œ ì •ë¥˜ì†Œ ìˆ˜: {optimal_stations}ê°œ")
        print(f"ğŸš² ì •ë¥˜ì†Œë‹¹ ì˜ˆìƒ ì›” ì´ìš©ëŸ‰: {predicted_usage_per_station:.0f}ê±´")
        print(f"ğŸ“ˆ ì›”ê°„ ì´ ì˜ˆìƒ ì´ìš©ëŸ‰: {optimal_stations * predicted_usage_per_station:,.0f}ê±´")
        print(f"ğŸ“… ì—°ê°„ ì´ ì˜ˆìƒ ì´ìš©ëŸ‰: {optimal_stations * predicted_usage_per_station * 12:,.0f}ê±´")
        
        print(f"\nğŸ† ìƒìœ„ 8ê°œ ìš°ì„  ì„¤ì¹˜ ìœ„ì¹˜:")
        for i, loc in enumerate(recommended_locations[:8], 1):
            print(f"{i:2d}. {loc['ìœ„ì¹˜']} ({loc['ì¹´í…Œê³ ë¦¬']}) - {loc['ì˜ˆìƒì´ìš©']}")
            print(f"     ğŸ“ {loc['ì£¼ì†Œ']}")
        
        self.daegu_results = results
        return results
    
    def visualize_results(self):
        """ê²°ê³¼ ì‹œê°í™”"""
        print("\n=== ê²°ê³¼ ì‹œê°í™” ===")
        
        if not hasattr(self, 'daegu_results'):
            print("âŒ ì¶”ì²œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë”°ë¦‰ì´ ì •ë¥˜ì†Œ ì¶”ì²œ ë¶„ì„ ê²°ê³¼', fontsize=14, fontweight='bold')
        
        # 1. ì„œìš¸ ì‹¤ì œ ë°ì´í„° vs ì„ì‹œ ë°ì´í„° ë¹„êµ
        if hasattr(self, 'seoul_district_features'):
            df = self.seoul_district_features
            group_means = df.groupby('ê·¸ë£¹')['ì •ë¥˜ì†Œë‹¹_ì´ìš©ëŸ‰'].mean()
            
            axes[0,0].bar(group_means.index, group_means.values, 
                         color=['blue', 'green', 'red'])
            axes[0,0].set_title('ì„œìš¸ ê·¸ë£¹ë³„ í‰ê·  ì •ë¥˜ì†Œë‹¹ ì´ìš©ëŸ‰')
            axes[0,0].set_ylabel('ì •ë¥˜ì†Œë‹¹ ì´ìš©ëŸ‰')
            
            for i, v in enumerate(group_means.values):
                axes[0,0].text(i, v + 5, f'{v:.0f}', ha='center', fontweight='bold')
        
        # 2. ëŒ€êµ¬ ì›”ë³„ ì˜ˆìƒ ì´ìš©ëŸ‰
        monthly_data = self.daegu_results['monthly_predictions']
        months = [item['ì›”'] for item in monthly_data]
        usage = [item['ì´_ì˜ˆìƒì´ìš©'] for item in monthly_data]
        
        axes[0,1].plot(months, usage, marker='o', linewidth=2, markersize=6, color='purple')
        axes[0,1].set_title('ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì›”ë³„ ì˜ˆìƒ ì´ ì´ìš©ëŸ‰')
        axes[0,1].set_xlabel('ì›”')
        axes[0,1].set_ylabel('ì˜ˆìƒ ì´ìš©ê±´ìˆ˜')
        axes[0,1].grid(True, alpha=0.3)
        
        # 3. ì¹´í…Œê³ ë¦¬ë³„ ì •ë¥˜ì†Œ ë¶„í¬
        categories = {}
        for loc in self.daegu_results['recommended_locations']:
            cat = loc['ì¹´í…Œê³ ë¦¬']
            categories[cat] = categories.get(cat, 0) + 1
        
        axes[1,0].pie(categories.values(), labels=categories.keys(), autopct='%1.1f%%', startangle=90)
        axes[1,0].set_title('ì¶”ì²œ ì •ë¥˜ì†Œ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬')
        
        # 4. ì˜ˆìƒ ì´ìš©ìˆ˜ì¤€ë³„ ë¶„í¬
        usage_levels = {'ë§¤ìš°ë†’ìŒ': 0, 'ë†’ìŒ': 0, 'ì¤‘ê°„': 0, 'ë‚®ìŒ': 0}
        for loc in self.daegu_results['recommended_locations']:
            level = loc['ì˜ˆìƒì´ìš©']
            usage_levels[level] += 1
        
        colors = ['#FF6B6B', '#FFD93D', '#6BCF7F', '#4D96FF']
        bars = axes[1,1].bar(usage_levels.keys(), usage_levels.values(), color=colors)
        axes[1,1].set_title('ì˜ˆìƒ ì´ìš©ìˆ˜ì¤€ë³„ ì •ë¥˜ì†Œ ë¶„í¬')
        axes[1,1].set_ylabel('ì •ë¥˜ì†Œ ê°œìˆ˜')
        
        for bar, count in zip(bars, usage_levels.values()):
            if count > 0:
                axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                             str(count), ha='center', fontweight='bold')
        
        plt.tight_layout()
        plt.show()
        
        self.print_final_summary()
    
    def print_final_summary(self):
        """ìµœì¢… ë¶„ì„ ìš”ì•½"""
        print("\n" + "="*70)
        print("        ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë”°ë¦‰ì´ ì •ë¥˜ì†Œ ì¶”ì²œ ë¶„ì„ ìµœì¢… ìš”ì•½")
        print("="*70)
        
        # ì„œìš¸ ë°ì´í„° ë¶„ì„ ê²°ê³¼
        if hasattr(self, 'seoul_district_features'):
            total_districts = len(self.seoul_district_features)
            valid_districts = len(self.seoul_district_features[self.seoul_district_features['ì´_ì´ìš©ê±´ìˆ˜'] > 0])
            print(f"\nğŸ“Š ì„œìš¸ ë°ì´í„° ë¶„ì„:")
            print(f"   ì „ì²´ êµ¬ ìˆ˜: {total_districts}ê°œ")
            print(f"   ì‹¤ì œ ë°ì´í„° êµ¬: {valid_districts}ê°œ")
            print(f"   ë°ì´í„° í™œìš©ë¥ : {valid_districts/total_districts*100:.1f}%")
        
        # ëª¨ë¸ ì„±ëŠ¥
        if hasattr(self, 'models') and self.models:
            best_score = max([model.get('score', 0) for model in self.models.values()])
            print(f"\nğŸ¤– ëª¨ë¸ ì„±ëŠ¥:")
            print(f"   ìµœê³  RÂ² ì ìˆ˜: {best_score:.3f}")
            if best_score > 0.8:
                print("   â†’ ë†’ì€ ì˜ˆì¸¡ ì •í™•ë„")
            elif best_score > 0.6:
                print("   â†’ ë³´í†µ ì˜ˆì¸¡ ì •í™•ë„")
            else:
                print("   â†’ ë‚®ì€ ì˜ˆì¸¡ ì •í™•ë„ (ì„ì‹œ ë°ì´í„° ì‚¬ìš©)")
        
        # ëŒ€êµ¬ ì¶”ì²œ ê²°ê³¼
        if hasattr(self, 'daegu_results'):
            results = self.daegu_results
            print(f"\nğŸ¯ ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì¶”ì²œ ê²°ê³¼:")
            print(f"   ì¶”ì²œ ì •ë¥˜ì†Œ ìˆ˜: {results['optimal_stations']}ê°œ")
            print(f"   ì •ë¥˜ì†Œë‹¹ ì›” ì´ìš©ëŸ‰: {results['predicted_usage_per_station']}ê±´")
            print(f"   ì›”ê°„ ì´ ì˜ˆìƒ ì´ìš©ëŸ‰: {results['total_monthly_usage']:,}ê±´")
            print(f"   ì—°ê°„ ì´ ì˜ˆìƒ ì´ìš©ëŸ‰: {results['total_monthly_usage'] * 12:,}ê±´")
            
            print(f"\nğŸ† TOP 3 ìš°ì„  ì„¤ì¹˜ ìœ„ì¹˜:")
            for i, loc in enumerate(results['recommended_locations'][:3], 1):
                print(f"   {i}. {loc['ìœ„ì¹˜']} ({loc['ì¹´í…Œê³ ë¦¬']}) - {loc['ì˜ˆìƒì´ìš©']}")
        
        # ëŒ€êµ¬ ì¸êµ¬ íŠ¹ì„±
        if hasattr(self, 'daegu_features'):
            print(f"\nğŸ˜ï¸ ëŒ€êµ¬ ìˆ˜ì„±êµ¬ íŠ¹ì„±:")
            print(f"   ì´ ì¸êµ¬ìˆ˜: {self.daegu_features['ì´ì¸êµ¬ìˆ˜']:,}ëª…")
            print(f"   ì¸êµ¬ë°€ë„: {self.daegu_features['ì¸êµ¬ë°€ë„']:,.0f}ëª…/kmÂ²")
        
        print("\nğŸ’¡ ê²°ë¡ :")
        print("   - ì„œëŒ€êµ¬ì—­ê³¼ ì„±ì„œí„°ë¯¸ë„ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ êµí†µ ê±°ì  ìš°ì„  ì„¤ì¹˜")
        print("   - ê³„ëª…ëŒ€í•™êµì™€ ìƒì—…ì‹œì„¤ì„ ì—°ê³„í•œ ì´ìš© í™œì„±í™”")
        print("   - ë‹¨ê³„ì  í™•ì¥ì„ í†µí•œ ë„¤íŠ¸ì›Œí¬ êµ¬ì¶• ê¶Œì¥")
        
        print("\n" + "="*70)
    
    def run_complete_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš´â€â™‚ï¸ ìˆ˜ì •ëœ ì„œìš¸ ë”°ë¦‰ì´ ë°ì´í„° ê¸°ë°˜ ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì •ë¥˜ì†Œ ì¶”ì²œ ì‹œìŠ¤í…œ")
        print("="*80)
        
        steps = [
            ("ì‹¤ì œ ë°ì´í„° ë¡œë”©", self.load_real_data),
            ("ì„œìš¸ ë°ì´í„° ì²˜ë¦¬", self.process_seoul_data),
            ("ì„œìš¸ êµ¬ë³„ íŠ¹ì„± ìƒì„±", self.create_district_features),
            ("ëŒ€êµ¬ ì¸êµ¬ ë°ì´í„° íŒŒì‹±", self.parse_daegu_population),
            ("ML ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦", self.train_ml_models),
            ("ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì •ë¥˜ì†Œ ì¶”ì²œ", self.predict_daegu_stations),
            ("ê²°ê³¼ ì‹œê°í™”", self.visualize_results)
        ]
        
        success_count = 0
        for step_name, step_func in steps:
            print(f"\nğŸ”„ {step_name} ì§„í–‰ ì¤‘...")
            try:
                result = step_func()
                if result:
                    print(f"âœ… {step_name} ì™„ë£Œ")
                    success_count += 1
                else:
                    print(f"âš ï¸ {step_name} ë¶€ë¶„ ì™„ë£Œ")
                    success_count += 0.5
            except Exception as e:
                print(f"âŒ {step_name} ì˜¤ë¥˜: {e}")
        
        print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ! {success_count}/{len(steps)} ë‹¨ê³„ ì„±ê³µ")
        return success_count >= len(steps) * 0.8

# ì‹¤í–‰
if __name__ == "__main__":
    analyzer = WorkingSeoulTtareungyiAnalyzer()
    analyzer.run_complete_analysis()