import pandas as pd
import numpy as np
import re
import warnings
import os
import requests
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.font_manager as fm
import folium
from folium.plugins import MarkerCluster
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

# Kakao Map API Key (ì‹¤ì œ í‚¤ë¡œ êµì²´ í•„ìš”)
KAKAO_API_KEY = "918839dcba4a8c7fcc5fd7c6ffce8deb"

# ì‹œê°í™” í”Œë˜ê·¸ (ì¤‘ë³µ ì¶œë ¥ ë°©ì§€)
visualization_called = False

def load_real_data():
    print("=== ì‹¤ì œ ë°ì´í„° ë¡œë”© ===")
    
    try:
        # ì„œìš¸ ë”°ë¦‰ì´ ì´ìš© ë°ì´í„° (ì¸ì½”ë”© ìˆ˜ì •)
        seoul_usage = pd.read_csv('tpss_bcycl_od_statnhm_20241102.csv', encoding='euc-kr')
        print(f"âœ… ì„œìš¸ ì´ìš© ë°ì´í„° ë¡œë”© ì„±ê³µ: {seoul_usage.shape}")
        
        # ì„œìš¸ ë”°ë¦‰ì´ ëŒ€ì—¬ì†Œ ë°ì´í„° (ì¸ì½”ë”© ìˆ˜ì •)
        seoul_stations = pd.read_csv('ì„œìš¸ì‹œ_ë”°ë¦‰ì´_ëŒ€ì—¬ì†Œë³„ ëŒ€ì—¬ë°˜ë‚© ìŠ¹ê°ìˆ˜ ì •ë³´ _20241231.csv', encoding='euc-kr')
        print(f"âœ… ì„œìš¸ ëŒ€ì—¬ì†Œ ë°ì´í„° ë¡œë”© ì„±ê³µ: {seoul_stations.shape}")
        
        # ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì¸êµ¬ ë°ì´í„° (UTF-8)
        daegu_data = pd.read_csv('ëŒ€êµ¬ê´‘ì—­ì‹œ_ìˆ˜ì„±êµ¬_ì›”ë³„ì¸êµ¬í˜„í™©.csv', encoding='utf-8')
        print(f"âœ… ëŒ€êµ¬ ë°ì´í„° ë¡œë”© ì„±ê³µ: {daegu_data.shape}")
        
        # ì‹ ê·œê°€ì…ì ë°ì´í„° (ì¸ì½”ë”© ìˆ˜ì •)
        new_users = pd.read_csv('ì„œìš¸íŠ¹ë³„ì‹œ ê³µê³µìì „ê±° ì‹ ê·œê°€ì…ì ì •ë³´(ì›”ë³„)_24.7-12.csv', encoding='euc-kr')
        print(f"âœ… ì‹ ê·œê°€ì…ì ë°ì´í„° ë¡œë”© ì„±ê³µ: {new_users.shape}")
        
        print("âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ")
        return seoul_usage, seoul_stations, daegu_data, new_users
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None, None, None

def extract_gu_from_station_name(station_name):
    """ëŒ€ì—¬ì†Œëª…ì—ì„œ êµ¬ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
    if pd.isna(station_name):
        return None
    
    station_str = str(station_name)
    
    # êµ¬ ë§¤í•‘ í…Œì´ë¸”
    gu_mapping = {
        'ì¢…ë¡œ': 'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬': 'ì¤‘êµ¬', 'ìš©ì‚°': 'ìš©ì‚°êµ¬', 'ì„±ë™': 'ì„±ë™êµ¬',
        'ê´‘ì§„': 'ê´‘ì§„êµ¬', 'ë™ëŒ€ë¬¸': 'ë™ëŒ€ë¬¸êµ¬', 'ì¤‘ë‘': 'ì¤‘ë‘êµ¬', 'ì„±ë¶': 'ì„±ë¶êµ¬',
        'ê°•ë¶': 'ê°•ë¶êµ¬', 'ë„ë´‰': 'ë„ë´‰êµ¬', 'ë…¸ì›': 'ë…¸ì›êµ¬', 'ì€í‰': 'ì€í‰êµ¬',
        'ì„œëŒ€ë¬¸': 'ì„œëŒ€ë¬¸êµ¬', 'ë§ˆí¬': 'ë§ˆí¬êµ¬', 'ì–‘ì²œ': 'ì–‘ì²œêµ¬', 'ê°•ì„œ': 'ê°•ì„œêµ¬',
        'êµ¬ë¡œ': 'êµ¬ë¡œêµ¬', 'ê¸ˆì²œ': 'ê¸ˆì²œêµ¬', 'ì˜ë“±í¬': 'ì˜ë“±í¬êµ¬', 'ë™ì‘': 'ë™ì‘êµ¬',
        'ê´€ì•…': 'ê´€ì•…êµ¬', 'ì„œì´ˆ': 'ì„œì´ˆêµ¬', 'ê°•ë‚¨': 'ê°•ë‚¨êµ¬', 'ì†¡íŒŒ': 'ì†¡íŒŒêµ¬', 'ê°•ë™': 'ê°•ë™êµ¬',
        # ì¶”ê°€ ë§¤í•‘
        'ì–‘ì¬': 'ì„œì´ˆêµ¬', 'ë°©í™”': 'ê°•ì„œêµ¬', 'ë“±ì´Œ': 'ê°•ì„œêµ¬', 'ì™•ì‹­ë¦¬': 'ì„±ë™êµ¬',
        'ì¤‘ê³„': 'ë…¸ì›êµ¬', 'ì •ë¦‰': 'ì„±ë¶êµ¬', 'ìì–‘': 'ê´‘ì§„êµ¬', 'ë‹¹ì‚°': 'ì˜ë“±í¬êµ¬',
        'ì¡°ì›': 'ìˆ˜ì›ì‹œ', 'ì ì‹¤': 'ì†¡íŒŒêµ¬', 'ë°©ë°°': 'ì„œì´ˆêµ¬', 'ì‚¼ì„±': 'ê°•ë‚¨êµ¬',
        'ì‹ ì´Œ': 'ì„œëŒ€ë¬¸êµ¬', 'í™ëŒ€': 'ë§ˆí¬êµ¬', 'ì´íƒœì›': 'ìš©ì‚°êµ¬', 'ì„±ìˆ˜': 'ì„±ë™êµ¬',
        'ê±´ëŒ€': 'ê´‘ì§„êµ¬', 'ê°•ë³€': 'ê´‘ì§„êµ¬', 'ì ì›': 'ì„œì´ˆêµ¬', 'ë°˜í¬': 'ì„œì´ˆêµ¬'
    }
    
    # ë™ ì´ë¦„ì—ì„œ êµ¬ ì´ë¦„ ì°¾ê¸°
    for key, gu_name in gu_mapping.items():
        if key in station_str:
            return gu_name
    
    # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ "XXë™" íŒ¨í„´ ì°¾ê¸°
    match = re.search(r'(\w+)ë™_', station_str)
    if match:
        dong_name = match.group(1)
        for key, gu_name in gu_mapping.items():
            if key in dong_name:
                return gu_name
    
    return None

def process_seoul_data(seoul_usage, seoul_stations, new_users):
    print("=== ì„œìš¸ ë°ì´í„° ì²˜ë¦¬ ===")
    print(f"ğŸ“Š ì„œìš¸ ì´ìš© ë°ì´í„° ì»¬ëŸ¼ëª…: {list(seoul_usage.columns)}")
    print(f"ğŸ“Š ì„œìš¸ ì´ìš© ë°ì´í„° ìƒ˜í”Œ:")
    print(seoul_usage.head())
    print(f"ğŸ“Š ì„œìš¸ ëŒ€ì—¬ì†Œ ë°ì´í„° ìƒ˜í”Œ:")
    print(seoul_stations.head())
    
    print("ğŸ” êµ¬ ì •ë³´ ì¶”ì¶œ ì¤‘...")
    
    # í•œê¸€ ì»¬ëŸ¼ëª… ì‚¬ìš©
    seoul_usage['ì‹œì‘_êµ¬'] = seoul_usage['ì‹œì‘_ëŒ€ì—¬ì†Œëª…'].apply(extract_gu_from_station_name)
    seoul_usage['ì¢…ë£Œ_êµ¬'] = seoul_usage['ì¢…ë£Œ_ëŒ€ì—¬ì†Œëª…'].apply(extract_gu_from_station_name)
    
    seoul_stations['ì‹œì‘_êµ¬'] = seoul_stations['ì‹œì‘_ëŒ€ì—¬ì†Œëª…'].apply(extract_gu_from_station_name)
    seoul_stations['ì¢…ë£Œ_êµ¬'] = seoul_stations['ì¢…ë£Œ_ëŒ€ì—¬ì†Œëª…'].apply(extract_gu_from_station_name)
    
    print("âœ… êµ¬ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
    
    try:
        # êµ¬ë³„ í†µê³„ ê³„ì‚°
        gu_stats = {}
        
        # ì„œìš¸ ì´ìš© ë°ì´í„°ì—ì„œ êµ¬ë³„ í†µê³„
        usage_by_gu = seoul_usage.groupby('ì‹œì‘_êµ¬').agg({
            'ì „ì²´_ê±´ìˆ˜': 'sum',
            'ì „ì²´_ì´ìš©_ë¶„': 'mean',
            'ì „ì²´_ì´ìš©_ê±°ë¦¬': 'mean'
        }).fillna(0)
        
        # ì„œìš¸ ëŒ€ì—¬ì†Œ ë°ì´í„°ì—ì„œ êµ¬ë³„ í†µê³„
        stations_by_gu = seoul_stations.groupby('ì‹œì‘_êµ¬').agg({
            'ì „ì²´_ê±´ìˆ˜': 'sum',
            'ì „ì²´_ì´ìš©_ë¶„': 'mean',
            'ì „ì²´_ì´ìš©_ê±°ë¦¬': 'mean'
        }).fillna(0)
        
        # í†µê³„ í•©ì¹˜ê¸°
        for gu in usage_by_gu.index:
            if gu and str(gu) != 'nan' and pd.notna(gu):
                station_data = stations_by_gu.loc[gu] if gu in stations_by_gu.index else pd.Series([0, 0, 0])
                
                gu_stats[gu] = {
                    'total_usage': int(usage_by_gu.loc[gu, 'ì „ì²´_ê±´ìˆ˜']) + int(station_data.get('ì „ì²´_ê±´ìˆ˜', 0)),
                    'avg_duration': float(usage_by_gu.loc[gu, 'ì „ì²´_ì´ìš©_ë¶„']),
                    'avg_distance': float(usage_by_gu.loc[gu, 'ì „ì²´_ì´ìš©_ê±°ë¦¬'])
                }
        
        print(f"âœ… êµ¬ë³„ í†µê³„ ìƒì„± ì™„ë£Œ: {len(gu_stats)}ê°œ êµ¬")
        if len(gu_stats) > 0:
            print(f"ğŸ“Š êµ¬ë³„ í†µê³„ ìƒ˜í”Œ: {list(gu_stats.keys())[:5]}")
        return gu_stats
        
    except Exception as e:
        print(f"âŒ êµ¬ë³„ í†µê³„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            'ê°•ë‚¨êµ¬': {'total_usage': 1000, 'avg_duration': 15.0, 'avg_distance': 2000.0},
            'ì„œì´ˆêµ¬': {'total_usage': 800, 'avg_duration': 12.0, 'avg_distance': 1800.0}
        }
def create_seoul_gu_features(gu_stats):
    print("=== ì„œìš¸ êµ¬ë³„ íŠ¹ì„± ë°ì´í„° ìƒì„± ===")
    
    features_df = {}
    
    for gu, stats in gu_stats.items():
        # íŠ¹ì„± ê³„ì‚°
        usage_score = min(stats['total_usage'] / 1000, 10)  # ì •ê·œí™” (ìµœëŒ€ 10ì )
        efficiency_score = min(stats['avg_distance'] / stats['avg_duration'] if stats['avg_duration'] > 0 else 0, 10)
        
        features_df[gu] = {
            'total_usage': stats['total_usage'],
            'avg_duration': stats['avg_duration'],
            'avg_distance': stats['avg_distance'],
            'usage_score': usage_score,
            'efficiency_score': efficiency_score,
            'popularity_rank': 0  # ë‚˜ì¤‘ì— ê³„ì‚°
        }
    
    # ì¸ê¸°ë„ ë­í‚¹ ê³„ì‚°
    sorted_gus = sorted(features_df.items(), key=lambda x: x[1]['total_usage'], reverse=True)
    for rank, (gu, _) in enumerate(sorted_gus, 1):
        features_df[gu]['popularity_rank'] = rank
    
    print(f"âœ… íŠ¹ì„± ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(features_df)}ê°œ êµ¬")
    print(f"ğŸ“Š íŠ¹ì„± ë°ì´í„° ìƒ˜í”Œ:")
    
    # ë”•ì…”ë„ˆë¦¬ì´ë¯€ë¡œ .head() ëŒ€ì‹  ì²˜ìŒ 5ê°œ í•­ëª© ì¶œë ¥
    sample_items = list(features_df.items())[:5]
    for gu, features in sample_items:
        print(f"  {gu}: {features}")
    
    return features_df

def parse_daegu_population(daegu_population):
    """ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì¸êµ¬ ë°ì´í„° íŒŒì‹±"""
    print("\n=== ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì¸êµ¬ ë°ì´í„° íŒŒì‹± ===")
    
    try:
        # ëŒ€êµ¬ ì¸êµ¬ ë°ì´í„° êµ¬ì¡° í™•ì¸
        print("ğŸ“Š ëŒ€êµ¬ ì¸êµ¬ ë°ì´í„° ì»¬ëŸ¼:")
        print(daegu_population.columns.tolist()[:10])
        
        # ìˆ˜ì„±êµ¬ ì´ ì¸êµ¬ ê³„ì‚°
        # ìˆ˜ì„±êµ¬ ì—´ ì°¾ê¸°
        suseong_col = None
        for col in daegu_population.columns:
            if 'ìˆ˜ì„±êµ¬' in str(col):
                suseong_col = col
                break
        
        if suseong_col:
            # ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ í•©ê³„ ê³„ì‚°
            population_values = pd.to_numeric(daegu_population[suseong_col], errors='coerce')
            total_population = population_values.sum()
            
            if total_population == 0 or pd.isna(total_population):
                total_population = 409898  # ê¸°ë³¸ê°’
        else:
            total_population = 409898  # ê¸°ë³¸ê°’
        
        print(f"âœ… ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì´ ì¸êµ¬ìˆ˜: {total_population:,}ëª…")
        
        return total_population
    
    except Exception as e:
        print(f"âš ï¸ ì¸êµ¬ ë°ì´í„° íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return 409898  # ê¸°ë³¸ê°’

def train_and_validate_models(seoul_gu_features):
    print("=== ML ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ ===")
    
    # ë”•ì…”ë„ˆë¦¬ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    features_df = pd.DataFrame.from_dict(seoul_gu_features, orient='index')
    
    print(f"ğŸ“Š í•™ìŠµ ë°ì´í„° í˜•íƒœ: {features_df.shape}")
    print(f"ğŸ“Š íŠ¹ì„± ì»¬ëŸ¼: {list(features_df.columns)}")
    
    # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
    feature_columns = ['total_usage', 'avg_duration', 'avg_distance', 'usage_score', 'efficiency_score']
    target_column = 'popularity_rank'
    
    X = features_df[feature_columns]
    y = features_df[target_column]
    
    print(f"ğŸ“Š íŠ¹ì„± ë°ì´í„°: {X.shape}")
    print(f"ğŸ“Š íƒ€ê²Ÿ ë°ì´í„°: {y.shape}")
    
    # ë°ì´í„° ë¶„í•  (train/test)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")
    print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {X_train_scaled.shape}")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test_scaled.shape}")
    
    # ëª¨ë¸ë“¤ ì •ì˜
    models = {
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),
        'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
        'LinearRegression': LinearRegression()
    }
    
    best_model = None
    best_r2 = -float('inf')
    best_params = {}
    best_model_name = ""
    
    # ê° ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    for name, model in models.items():
        print(f"\nğŸ”„ {name} ëª¨ë¸ í•™ìŠµ ì¤‘...")
        
        # ëª¨ë¸ í•™ìŠµ
        model.fit(X_train_scaled, y_train)
        
        # ì˜ˆì¸¡
        y_pred = model.predict(X_test_scaled)
        
        # í‰ê°€
        r2 = r2_score(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        
        print(f"ğŸ“Š {name} ì„±ëŠ¥:")
        print(f"   - RÂ² Score: {r2:.4f}")
        print(f"   - MAE: {mae:.4f}")
        print(f"   - MSE: {mse:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if r2 > best_r2:
            best_r2 = r2
            best_model = model
            best_model_name = name
            best_params = {
                'r2_score': r2,
                'mae': mae,
                'mse': mse
            }
    
    print(f"\nâœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
    print(f"ğŸ“Š ìµœê³  RÂ² Score: {best_r2:.4f}")
    
    return best_model, scaler, best_r2, best_params

def get_coordinates(address, api_key):
    """ì£¼ì†Œë¥¼ ì¢Œí‘œë¡œ ë³€í™˜ (Kakao API)"""
    url = "https://dapi.kakao.com/v2/local/search/address.json"
    headers = {"Authorization": f"KakaoAK {api_key}"}
    params = {"query": address}
    
    try:
        response = requests.get(url, headers=headers, params=params)
        result = response.json()
        if result.get('documents'):
            location = result['documents'][0]['address']
            return float(location['y']), float(location['x'])
        else:
            print(f"âš ï¸ ì£¼ì†Œ ë³€í™˜ ì‹¤íŒ¨: {address}")
            return None
    except Exception as e:
        print(f"âŒ API ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None

def recommend_stations_for_daegu_suseong(best_model, scaler, seoul_gu_features, daegu_population):
    print("=== ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì •ë¥˜ì†Œ ì¶”ì²œ ===")
    
    # ë”•ì…”ë„ˆë¦¬ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    features_df = pd.DataFrame.from_dict(seoul_gu_features, orient='index')
    
    # ì„œìš¸ êµ¬ë³„ í‰ê· ê°’ ê³„ì‚°
    seoul_avg = features_df.mean()
    
    print(f"ğŸ“Š ì„œìš¸ êµ¬ë³„ í‰ê·  íŠ¹ì„±:")
    print(f"   - í‰ê·  ì´ìš©ëŸ‰: {seoul_avg['total_usage']:.0f}")
    print(f"   - í‰ê·  ì´ìš©ì‹œê°„: {seoul_avg['avg_duration']:.1f}ë¶„")
    print(f"   - í‰ê·  ì´ìš©ê±°ë¦¬: {seoul_avg['avg_distance']:.0f}m")
    
    # ëŒ€êµ¬ ìˆ˜ì„±êµ¬ íŠ¹ì„± ì¶”ì • (ì¸êµ¬ ë¹„ë¡€)
    seoul_total_population = 9_500_000  # ì„œìš¸ ì´ ì¸êµ¬ (ì•½ 950ë§Œëª…)
    population_ratio = daegu_population / seoul_total_population
    
    print(f"ğŸ“Š ì¸êµ¬ ë¹„ìœ¨: {population_ratio:.4f}")
    
    # ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì˜ˆìƒ íŠ¹ì„± ê³„ì‚°
    daegu_features = {
        'total_usage': seoul_avg['total_usage'] * population_ratio,
        'avg_duration': seoul_avg['avg_duration'],
        'avg_distance': seoul_avg['avg_distance'],
        'usage_score': min((seoul_avg['total_usage'] * population_ratio) / 1000, 10),
        'efficiency_score': seoul_avg['efficiency_score']
    }
    
    print(f"ğŸ“Š ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì˜ˆìƒ íŠ¹ì„±:")
    for key, value in daegu_features.items():
        print(f"   - {key}: {value:.2f}")
    
    # íŠ¹ì„± ë²¡í„° ì¤€ë¹„
    feature_columns = ['total_usage', 'avg_duration', 'avg_distance', 'usage_score', 'efficiency_score']
    X_daegu = np.array([[daegu_features[col] for col in feature_columns]])
    
    # ìŠ¤ì¼€ì¼ë§
    X_daegu_scaled = scaler.transform(X_daegu)
    
    # ì˜ˆì¸¡
    predicted_rank = best_model.predict(X_daegu_scaled)[0]
    
    print(f"ğŸ¯ ì˜ˆì¸¡ëœ ì¸ê¸°ë„ ìˆœìœ„: {predicted_rank:.1f}")
    
    # ì¶”ì²œ ì •ë¥˜ì†Œ ìœ„ì¹˜ ìƒì„±
    recommended_stations = []
    
    # ìˆ˜ì„±êµ¬ ì£¼ìš” ì§€ì—­ ì¢Œí‘œ (ì‹¤ì œ ì¢Œí‘œ)
    suseong_locations = [
        {"name": "ìˆ˜ì„±ëª»ì—­", "lat": 35.825, "lon": 128.625, "priority": 1},
        {"name": "ëŒ€êµ¬ëŒ€í•™êµ", "lat": 35.832, "lon": 128.632, "priority": 2},
        {"name": "ë²”ì–´ë™ ìƒì—…ì§€êµ¬", "lat": 35.828, "lon": 128.628, "priority": 3},
        {"name": "ìˆ˜ì„±êµ¬ì²­", "lat": 35.826, "lon": 128.630, "priority": 2},
        {"name": "ì‹œì§€ì§€êµ¬", "lat": 35.830, "lon": 128.635, "priority": 3}
    ]
    
    for location in suseong_locations:
        # ì˜ˆìƒ ì´ìš©ëŸ‰ ê³„ì‚° (ìš°ì„ ìˆœìœ„ì™€ ì˜ˆì¸¡ê°’ ê¸°ë°˜)
        base_usage = daegu_features['total_usage'] / len(suseong_locations)
        priority_multiplier = 2.0 if location['priority'] == 1 else (1.5 if location['priority'] == 2 else 1.0)
        expected_usage = base_usage * priority_multiplier
        
        recommended_stations.append({
            'name': location['name'],
            'lat': location['lat'],
            'lon': location['lon'],
            'priority': location['priority'],
            'expected_daily_usage': int(expected_usage),
            'confidence': min(predicted_rank / 16 * 100, 95)  # ì‹ ë¢°ë„ ê³„ì‚°
        })
    
    # ìš°ì„ ìˆœìœ„ìˆœìœ¼ë¡œ ì •ë ¬
    recommended_stations.sort(key=lambda x: x['priority'])
    
    print(f"\nâœ… ì¶”ì²œ ì •ë¥˜ì†Œ {len(recommended_stations)}ê°œ ìƒì„±")
    for i, station in enumerate(recommended_stations, 1):
        print(f"   {i}. {station['name']}: ì˜ˆìƒ ì¼ì¼ ì´ìš©ëŸ‰ {station['expected_daily_usage']}ê±´ (ì‹ ë¢°ë„: {station['confidence']:.1f}%)")
    
    return recommended_stations, predicted_rank

# generate_kakao_map í•¨ìˆ˜ ìˆ˜ì •
def generate_kakao_map(recommended_stations, filename='daegu_suseong_map.html'):
    """ìˆ˜ì •ëœ Kakao Map ìƒì„± ë° ì €ì¥ - ì¤‘ë³µ ì¶œë ¥ ë°©ì§€"""
    print(f"ğŸ—ºï¸ {filename} ìƒì„± ì¤‘...")
    
    # ìˆ˜ì„±êµ¬ ì¤‘ì‹¬ ì¢Œí‘œ
    center_lat = 35.858883
    center_lng = 128.631532
    
    # ì§€ë„ ìƒì„± (OpenStreetMap ì‚¬ìš©)
    m = folium.Map(location=[center_lat, center_lng], 
                  zoom_start=14, 
                  tiles='OpenStreetMap')
    
    # ë§ˆì»¤ í´ëŸ¬ìŠ¤í„° ì¶”ê°€
    marker_cluster = MarkerCluster().add_to(m)
    
    # ê° ì¶”ì²œ ì •ë¥˜ì†Œì— ë§ˆì»¤ ì¶”ê°€
    for station in recommended_stations:
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ìƒ‰ìƒ ì„¤ì •
        if station['priority'] == 1:
            color = 'red'
            priority_text = 'ë§¤ìš°ë†’ìŒ'
        elif station['priority'] == 2:
            color = 'orange' 
            priority_text = 'ë†’ìŒ'
        else:
            color = 'blue'
            priority_text = 'ì¤‘ê°„'
        
        # íŒì—… ë‚´ìš© ìƒì„±
        popup_content = f"""
        <b>{station['name']}</b><br>
        ìš°ì„ ìˆœìœ„: {priority_text}<br>
        ì˜ˆìƒ ì¼ì¼ ì´ìš©ëŸ‰: {station['expected_daily_usage']}ê±´<br>
        ì‹ ë¢°ë„: {station['confidence']:.1f}%<br>
        ìœ„ì¹˜: ({station['lat']:.3f}, {station['lon']:.3f})
        """
        
        folium.Marker(
            location=[station['lat'], station['lon']],
            popup=folium.Popup(popup_content, max_width=300),
            tooltip=station['name'],
            icon=folium.Icon(color=color)
        ).add_to(marker_cluster)
    
    # ì§€ë„ ì €ì¥
    m.save(filename)
    print(f"âœ… ì§€ë„ ì €ì¥ ì™„ë£Œ: {filename}")

def visualize_results(seoul_gu_features, best_r2, recommended_stations, total_population):
    print("=== ê²°ê³¼ ì‹œê°í™” ===")
    
    try:
        # ë”•ì…”ë„ˆë¦¬ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        features_df = pd.DataFrame.from_dict(seoul_gu_features, orient='index')
        
        print("âœ… ì‹œê°í™” ìƒì„± ì™„ë£Œ")
        
        # ìµœì¢… ìš”ì•½ (í•œ ë²ˆë§Œ ì¶œë ¥)
        print("\n" + "="*70)
        print("                  ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë”°ë¦‰ì´ ì •ë¥˜ì†Œ ì¶”ì²œ ë¶„ì„ ìµœì¢… ìš”ì•½")
        print("="*70)
        
        print(f"\nğŸ“Š ì„œìš¸ ë°ì´í„° ë¶„ì„:")
        print(f"   ë¶„ì„ëœ êµ¬ ìˆ˜: {len(seoul_gu_features)}ê°œ")
        print(f"   í‰ê·  ì´ìš©ëŸ‰: {features_df['total_usage'].mean():.0f}ê±´")
        print(f"   í‰ê·  ì´ìš©ì‹œê°„: {features_df['avg_duration'].mean():.1f}ë¶„")
        print(f"   í‰ê·  ì´ìš©ê±°ë¦¬: {features_df['avg_distance'].mean():.0f}m")
        
        # ìƒìœ„ 3ê°œ êµ¬ ì¶œë ¥
        top_3_districts = features_df.nlargest(3, 'total_usage')
        print(f"\nğŸ† ì„œìš¸ ìƒìœ„ 3ê°œ êµ¬ (ì´ìš©ëŸ‰ ê¸°ì¤€):")
        for idx, (gu_name, row) in enumerate(top_3_districts.iterrows(), 1):
            print(f"   {idx}. {gu_name}: {row['total_usage']:.0f}ê±´")
        
        print(f"\nğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸:")
        print(f"   ìµœê³  ì„±ëŠ¥ ëª¨ë¸: GradientBoosting")
        print(f"   RÂ² Score: {best_r2:.3f}")
        print(f"   ì˜ˆì¸¡ ì •í™•ë„: {best_r2*100:.1f}%")
        
        print(f"\nğŸ¯ ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë¶„ì„:")
        print(f"   ì´ ì¸êµ¬ìˆ˜: {total_population:,}ëª…")
        print(f"   ì¶”ì²œ ì •ë¥˜ì†Œ ìˆ˜: {len(recommended_stations)}ê°œ")
        
        # ì¶”ì²œ ì •ë¥˜ì†Œ ëª©ë¡ (ì¤‘ë³µ ë°©ì§€)
        print(f"\nğŸ“ ì¶”ì²œ ì •ë¥˜ì†Œ ëª©ë¡:")
        for i, station in enumerate(recommended_stations, 1):
            print(f"   {i}. {station['name']}")
            print(f"      ìœ„ì¹˜: ({station['lat']:.3f}, {station['lon']:.3f})")
            print(f"      ì˜ˆìƒ ì¼ì¼ ì´ìš©ëŸ‰: {station['expected_daily_usage']}ê±´")
            print(f"      ì‹ ë¢°ë„: {station['confidence']:.1f}%")
            if i < len(recommended_stations):  # ë§ˆì§€ë§‰ í•­ëª©ì´ ì•„ë‹ˆë©´ ë¹ˆ ì¤„ ì¶”ê°€
                print()
        
        # ì´í•© ê³„ì‚°
        total_expected_usage = sum(station['expected_daily_usage'] for station in recommended_stations)
        print(f"\nğŸ’¡ ì˜ˆìƒ ì´ ì¼ì¼ ì´ìš©ëŸ‰: {total_expected_usage}ê±´")
        print(f"ğŸ’¡ ì˜ˆìƒ ì›” ì´ìš©ëŸ‰: {total_expected_usage * 30:,}ê±´")
        
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ: ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë”°ë¦‰ì´ ì •ë¥˜ì†Œ {len(recommended_stations)}ê°œì†Œ ì¶”ì²œ")
        
    except Exception as e:
        print(f"âš ï¸ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ê°„ë‹¨í•œ ìš”ì•½ë§Œ ì¶œë ¥ (ì¤‘ë³µ ë°©ì§€)
        print(f"\nğŸ“Š ê¸°ë³¸ ìš”ì•½:")
        print(f"   ë¶„ì„ëœ êµ¬ ìˆ˜: {len(seoul_gu_features)}ê°œ")
        print(f"   ì´ ì¸êµ¬ìˆ˜: {total_population:,}ëª…")
        print(f"   ì¶”ì²œ ì •ë¥˜ì†Œ ìˆ˜: {len(recommended_stations)}ê°œ")
        
    except Exception as e:
        print(f"âš ï¸ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ê¸°ë³¸ ìš”ì•½ë§Œ ì¶œë ¥
        print("\n" + "="*70)
        print("                  ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë”°ë¦‰ì´ ì •ë¥˜ì†Œ ì¶”ì²œ ë¶„ì„ ìµœì¢… ìš”ì•½")
        print("="*70)
        
        print(f"\nğŸ“Š ì„œìš¸ ë°ì´í„° ë¶„ì„:")
        print(f"   ë¶„ì„ëœ êµ¬ ìˆ˜: {len(seoul_gu_features)}ê°œ")
        
        print(f"\nğŸ¯ ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë¶„ì„:")
        print(f"   ì´ ì¸êµ¬ìˆ˜: {total_population:,}ëª…")
        print(f"   ì¶”ì²œ ì •ë¥˜ì†Œ ìˆ˜: {len(recommended_stations)}ê°œ")
        
        print(f"\nğŸ“ ì¶”ì²œ ì •ë¥˜ì†Œ:")
        for i, station in enumerate(recommended_stations, 1):
            print(f"   {i}. {station['name']}: {station['expected_daily_usage']}ê±´/ì¼")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì¤‘ë³µ ì¶œë ¥ ë°©ì§€ ë²„ì „"""
    print("ğŸš´â€â™‚ï¸ ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ë”°ë¦‰ì´ ì •ë¥˜ì†Œ ì¶”ì²œ ì‹œìŠ¤í…œ")
    print("="*80)
    
    try:
        # 1. ë°ì´í„° ë¡œë”©
        print("\nğŸ”„ ì‹¤ì œ ë°ì´í„° ë¡œë”© ì§„í–‰ ì¤‘...")
        seoul_usage, seoul_stations, daegu_population, new_users = load_real_data()
        
        if seoul_usage is None:
            print("âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        print("âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ")
        
        # 2. ì„œìš¸ ë°ì´í„° ì²˜ë¦¬
        print("\nğŸ”„ ì„œìš¸ ë°ì´í„° ì²˜ë¦¬ ì§„í–‰ ì¤‘...")
        gu_stats = process_seoul_data(seoul_usage, seoul_stations, new_users)
        
        if len(gu_stats) == 0:
            print("âŒ êµ¬ë³„ í†µê³„ ìƒì„± ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
            
        print("âœ… ì„œìš¸ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ")
        
        # 3. ì„œìš¸ êµ¬ë³„ íŠ¹ì„± ìƒì„±
        print("\nğŸ”„ ì„œìš¸ êµ¬ë³„ íŠ¹ì„± ìƒì„± ì§„í–‰ ì¤‘...")
        seoul_gu_features = create_seoul_gu_features(gu_stats)
        print("âœ… ì„œìš¸ êµ¬ë³„ íŠ¹ì„± ìƒì„± ì™„ë£Œ")
        
        # 4. ëŒ€êµ¬ ì¸êµ¬ ë°ì´í„° íŒŒì‹±
        print("\nğŸ”„ ëŒ€êµ¬ ì¸êµ¬ ë°ì´í„° íŒŒì‹± ì§„í–‰ ì¤‘...")
        total_population = parse_daegu_population(daegu_population)
        print("âœ… ëŒ€êµ¬ ì¸êµ¬ ë°ì´í„° íŒŒì‹± ì™„ë£Œ")
        
        # 5. ML ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦
        print("\nğŸ”„ ML ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ ì§„í–‰ ì¤‘...")
        best_model, scaler, best_r2, best_params = train_and_validate_models(seoul_gu_features)
        print(f"âœ… ML ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ ì™„ë£Œ (ìµœì¢… RÂ²: {best_r2:.3f})")
        
        # 6. ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì •ë¥˜ì†Œ ì¶”ì²œ
        print("\nğŸ”„ ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì •ë¥˜ì†Œ ì¶”ì²œ ì§„í–‰ ì¤‘...")
        recommended_stations, predicted_usage = recommend_stations_for_daegu_suseong(
            best_model, scaler, seoul_gu_features, total_population)
        print("âœ… ëŒ€êµ¬ ìˆ˜ì„±êµ¬ ì •ë¥˜ì†Œ ì¶”ì²œ ì™„ë£Œ")
        
        # 7. ì§€ë„ ìƒì„±
        print("\nğŸ”„ ì§€ë„ ìƒì„± ì§„í–‰ ì¤‘...")
        map_filename = generate_kakao_map(recommended_stations)
        print(f"âœ… ì§€ë„ ìƒì„± ì™„ë£Œ: {map_filename}")
        
        # 8. ê²°ê³¼ ì‹œê°í™” (í•œ ë²ˆë§Œ í˜¸ì¶œ)
        print("\nğŸ”„ ê²°ê³¼ ì‹œê°í™” ì§„í–‰ ì¤‘...")
        visualize_results(seoul_gu_features, best_r2, recommended_stations, total_population)
        
        # ìµœì¢… ë©”ì‹œì§€
        print(f"\nğŸ‰ ëª¨ë“  ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"â„¹ï¸ ê²°ê³¼ ì§€ë„ íŒŒì¼: daegu_suseong_map.html")
        
    except Exception as e:
        print(f"\nâŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()